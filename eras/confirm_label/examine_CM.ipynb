{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_CM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMofQtYU03rbQNUqprUSeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_CM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxGMERAvX3P8"
      },
      "source": [
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0.0\n",
        "TEST_DATA_RATE = 0.25\n",
        "EXPERIMENTS_NUMBER = 6"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : \"/notice! this is designed at before each learning\",\n",
        "        \"momentum\" : 0.9,\n",
        "        \"decay\" : 1e-4,\n",
        "        \"nesterov\" : True\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 128,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : 0.001,\n",
        "#         \"epsilon\" : 1e-8,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 128,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                         tf.keras.losses.MeanAbsoluteError(),\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        epsilon=learningDict[\"optimizer\"][\"epsilon\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "b7d519ad-0a00-4bf9-8ccf-c20307797c2b"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/CM_experiment/cleaned_CM\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "d01da1e5-8c15-4216-85de-e5e019167d4f"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"Not reading such data\", each_data)\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/CM_experiment/cleaned_CM\n",
            "Not reading such data summary_CM_data0131.png\n",
            "________|------------ reading [ Data0131.npz ] as learning data.\n",
            "________|------------ reading [ Label0131.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])\n",
        "\n",
        "learning_data = np.array(learning_data)\n",
        "label_data = np.array(label_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータの教師ラベルに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rf5RcI7AzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5cea15e-88c8-404d-8c97-fb81d904024c"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_train) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_train)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_train)\n",
        "set_max = np.max(Y_train)\n",
        "sum = 0\n",
        "if LABEL_NOISE_RATE != 0:\n",
        "  for i in range(len(Y_train)):\n",
        "    if i in change_points:\n",
        "      print(\"No.\", i, \", original number is\", Y_train[i], end=\" -> \")\n",
        "      Y_train[i] = changed_number(Y_train[i], set_min, set_max)\n",
        "      print(\"changed number is\", Y_train[i])\n",
        "      sum += 1\n",
        "    else:\n",
        "      pass\n",
        "else:\n",
        "  print(\"No label noizes\")\n",
        "  \n",
        "if sum == len(change_points):\n",
        "  print(\"changed correctlly.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "change map: []\n",
            "the size: 0\n",
            "No label noizes\n",
            "changed correctlly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM3OSjUWEPH3"
      },
      "source": [
        "##教師ラベルをone-hotに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHvnUv6KDty_"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(Y_train, 4)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, 4)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3JzbydcYRv"
      },
      "source": [
        "def fix_loss_text(original):\n",
        "  result = re.sub(r\"<tensorflow\\.python\\.keras\\.losses\\.\", \"\", str(original))\n",
        "  result = re.sub(r\"\\sobject.+\", \"\", result)\n",
        "  return result"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQGLjvFcLRA"
      },
      "source": [
        "used_losses_set = []\n",
        "for each_loss in learningDict[\"compared_losses\"]:\n",
        "  used_losses_set.append(fix_loss_text(each_loss))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXCHuDWpGxz-",
        "outputId": "a0983b20-db43-4850-d9fa-70f0dbe25cf3"
      },
      "source": [
        "%%time\n",
        "obj_video_format = video_format()\n",
        "all_histories = [] # append EXPERIMENTS_NUMBER * used losses size\n",
        "\n",
        "start_time = time.time()\n",
        "for j in range(EXPERIMENTS_NUMBER):\n",
        "  seed_in_roop = seed + j\n",
        "  np.random.seed(seed_in_roop)\n",
        "  tf.random.set_seed(seed_in_roop)\n",
        "  print(\"runnng...\")\n",
        "\n",
        "  roop_histories = [] # append used losses size\n",
        "  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "\n",
        "    # 通知\n",
        "    all_loop_counter = str(j * len(learningDict[\"compared_losses\"]) + (i + 1))\n",
        "    all_loop_number = str(EXPERIMENTS_NUMBER * len(learningDict[\"compared_losses\"]))\n",
        "    \n",
        "    start_massage = \"Try the \" + all_loop_counter + \"/\" + all_loop_number + \" loop\\n\"\n",
        "    start_massage += \"Current used loss function is [\" + fix_loss_text(each_loss) + \"] = \" + str(i+1) +  \"/\" + str(len(learningDict[\"compared_losses\"]))\n",
        "\n",
        "    print(start_massage)\n",
        "    send_line_notify(start_massage)\n",
        "\n",
        "    try:\n",
        "      # モデル構築\n",
        "      model = make_model(obj_video_format)\n",
        "\n",
        "      if fix_loss_text(each_loss) == \"CategoricalCrossentropy\":\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-3)*2\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "      else:\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-2)\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "\n",
        "      model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=each_loss,\n",
        "            metrics=['acc'])\n",
        "      # 実行\n",
        "      \n",
        "      history = model.fit(\n",
        "            X_train, Y_train,\n",
        "            validation_data=(X_test, Y_test),\n",
        "            batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "            epochs=learningDict[\"theWay\"][\"epochs\"]\n",
        "            # verbose=0\n",
        "            )\n",
        "    except KeyboardInterrupt: \n",
        "      print(\"\\n\\nProcessing the KeyboardInterrupt\")\n",
        "\n",
        "    else:\n",
        "      roop_histories.append(history)\n",
        "      accumulation_time = str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "      finish_massage = \"Complete.\\n\"\n",
        "      finish_massage += \"The accumulation time is \" + str(accumulation_time) + \"\\n\"\n",
        "      finish_massage += \"Last val_acc is \" + str(history.history[\"val_acc\"][learningDict[\"theWay\"][\"epochs\"]-1])\n",
        "      print(finish_massage)\n",
        "      send_line_notify(finish_massage)\n",
        "    finally:\n",
        "      del model\n",
        "      keras.backend.clear_session()\n",
        "      gc.collect()\n",
        "      sleep(10)\n",
        "      clear_output()\n",
        "      print(\" and the model is erased.\")\n",
        "\n",
        "  #/for i\n",
        "  all_histories.append(roop_histories)\n",
        "\n",
        "#/for j\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " and the model is erased.\n",
            "Try the 2/18 loop\n",
            "Current used loss function is [MeanSquaredError] = 2/3\n",
            "sgd is used as a optimizer\n",
            "0.01\n",
            "Epoch 1/128\n",
            "6/6 [==============================] - 1s 192ms/step - loss: 0.1752 - acc: 0.4323 - val_loss: 0.1751 - val_acc: 0.4062\n",
            "Epoch 2/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 0.1705 - acc: 0.4178 - val_loss: 0.1821 - val_acc: 0.4062\n",
            "Epoch 3/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 0.1665 - acc: 0.4157 - val_loss: 0.1759 - val_acc: 0.4062\n",
            "Epoch 4/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 0.1628 - acc: 0.4645 - val_loss: 0.1685 - val_acc: 0.4219\n",
            "Epoch 5/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.1576 - acc: 0.5759 - val_loss: 0.1668 - val_acc: 0.4219\n",
            "Epoch 6/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.1452 - acc: 0.5713 - val_loss: 0.1718 - val_acc: 0.3750\n",
            "Epoch 7/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 0.1479 - acc: 0.4583 - val_loss: 0.1655 - val_acc: 0.4062\n",
            "Epoch 8/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 0.1442 - acc: 0.5862 - val_loss: 0.1698 - val_acc: 0.4219\n",
            "Epoch 9/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 0.1333 - acc: 0.5846 - val_loss: 0.1640 - val_acc: 0.5000\n",
            "Epoch 10/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 0.1320 - acc: 0.6432 - val_loss: 0.1616 - val_acc: 0.4844\n",
            "Epoch 11/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.1218 - acc: 0.6949 - val_loss: 0.1671 - val_acc: 0.4375\n",
            "Epoch 12/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.1174 - acc: 0.6603 - val_loss: 0.1595 - val_acc: 0.4531\n",
            "Epoch 13/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.1194 - acc: 0.6404 - val_loss: 0.1652 - val_acc: 0.4844\n",
            "Epoch 14/128\n",
            "6/6 [==============================] - 1s 187ms/step - loss: 0.0911 - acc: 0.8549 - val_loss: 0.1693 - val_acc: 0.4375\n",
            "Epoch 15/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0938 - acc: 0.8354 - val_loss: 0.1790 - val_acc: 0.4219\n",
            "Epoch 16/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 0.0881 - acc: 0.7858 - val_loss: 0.1616 - val_acc: 0.4531\n",
            "Epoch 17/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.0754 - acc: 0.8387 - val_loss: 0.1707 - val_acc: 0.4375\n",
            "Epoch 18/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.0560 - acc: 0.9352 - val_loss: 0.1775 - val_acc: 0.3906\n",
            "Epoch 19/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0502 - acc: 0.9280 - val_loss: 0.1703 - val_acc: 0.4844\n",
            "Epoch 20/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0458 - acc: 0.9277 - val_loss: 0.1625 - val_acc: 0.4688\n",
            "Epoch 21/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 0.0304 - acc: 0.9647 - val_loss: 0.1850 - val_acc: 0.3906\n",
            "Epoch 22/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 0.0294 - acc: 0.9641 - val_loss: 0.1633 - val_acc: 0.4531\n",
            "Epoch 23/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 0.0205 - acc: 0.9835 - val_loss: 0.1600 - val_acc: 0.5000\n",
            "Epoch 24/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0179 - acc: 0.9868 - val_loss: 0.1639 - val_acc: 0.4688\n",
            "Epoch 25/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0099 - acc: 0.9912 - val_loss: 0.1636 - val_acc: 0.4688\n",
            "Epoch 26/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.0067 - acc: 0.9965 - val_loss: 0.1644 - val_acc: 0.4844\n",
            "Epoch 27/128\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.1696 - val_acc: 0.5156\n",
            "Epoch 28/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.1696 - val_acc: 0.4688\n",
            "Epoch 29/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.1691 - val_acc: 0.4844\n",
            "Epoch 30/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1658 - val_acc: 0.5000\n",
            "Epoch 31/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1668 - val_acc: 0.5156\n",
            "Epoch 32/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1671 - val_acc: 0.5312\n",
            "Epoch 33/128\n",
            "6/6 [==============================] - 1s 188ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 0.5312\n",
            "Epoch 34/128\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1668 - val_acc: 0.5312\n",
            "Epoch 35/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 9.3517e-04 - acc: 1.0000 - val_loss: 0.1689 - val_acc: 0.5312\n",
            "Epoch 36/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 9.0348e-04 - acc: 1.0000 - val_loss: 0.1688 - val_acc: 0.5312\n",
            "Epoch 37/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 7.9807e-04 - acc: 1.0000 - val_loss: 0.1684 - val_acc: 0.5156\n",
            "Epoch 38/128\n",
            "6/6 [==============================] - 1s 175ms/step - loss: 6.7379e-04 - acc: 1.0000 - val_loss: 0.1700 - val_acc: 0.5312\n",
            "Epoch 39/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 6.6420e-04 - acc: 1.0000 - val_loss: 0.1700 - val_acc: 0.5312\n",
            "Epoch 40/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 6.0716e-04 - acc: 1.0000 - val_loss: 0.1706 - val_acc: 0.5312\n",
            "Epoch 41/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 6.1167e-04 - acc: 1.0000 - val_loss: 0.1706 - val_acc: 0.5312\n",
            "Epoch 42/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 5.2913e-04 - acc: 1.0000 - val_loss: 0.1704 - val_acc: 0.5312\n",
            "Epoch 43/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 5.0212e-04 - acc: 1.0000 - val_loss: 0.1710 - val_acc: 0.5312\n",
            "Epoch 44/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 4.5569e-04 - acc: 1.0000 - val_loss: 0.1711 - val_acc: 0.5312\n",
            "Epoch 45/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 4.4075e-04 - acc: 1.0000 - val_loss: 0.1709 - val_acc: 0.5312\n",
            "Epoch 46/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 4.2870e-04 - acc: 1.0000 - val_loss: 0.1713 - val_acc: 0.5312\n",
            "Epoch 47/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 3.9716e-04 - acc: 1.0000 - val_loss: 0.1715 - val_acc: 0.5312\n",
            "Epoch 48/128\n",
            "6/6 [==============================] - 1s 190ms/step - loss: 3.8657e-04 - acc: 1.0000 - val_loss: 0.1716 - val_acc: 0.5312\n",
            "Epoch 49/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 4.0611e-04 - acc: 1.0000 - val_loss: 0.1719 - val_acc: 0.5312\n",
            "Epoch 50/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 3.7031e-04 - acc: 1.0000 - val_loss: 0.1721 - val_acc: 0.5312\n",
            "Epoch 51/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 3.5488e-04 - acc: 1.0000 - val_loss: 0.1721 - val_acc: 0.5312\n",
            "Epoch 52/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 3.2325e-04 - acc: 1.0000 - val_loss: 0.1724 - val_acc: 0.5312\n",
            "Epoch 53/128\n",
            "6/6 [==============================] - 1s 165ms/step - loss: 3.3139e-04 - acc: 1.0000 - val_loss: 0.1724 - val_acc: 0.5312\n",
            "Epoch 54/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 3.3861e-04 - acc: 1.0000 - val_loss: 0.1719 - val_acc: 0.5156\n",
            "Epoch 55/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 2.9090e-04 - acc: 1.0000 - val_loss: 0.1732 - val_acc: 0.5312\n",
            "Epoch 56/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 2.9785e-04 - acc: 1.0000 - val_loss: 0.1730 - val_acc: 0.5312\n",
            "Epoch 57/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 2.7550e-04 - acc: 1.0000 - val_loss: 0.1730 - val_acc: 0.5312\n",
            "Epoch 58/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 2.8310e-04 - acc: 1.0000 - val_loss: 0.1729 - val_acc: 0.5156\n",
            "Epoch 59/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 2.4422e-04 - acc: 1.0000 - val_loss: 0.1734 - val_acc: 0.5312\n",
            "Epoch 60/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 2.5148e-04 - acc: 1.0000 - val_loss: 0.1732 - val_acc: 0.5312\n",
            "Epoch 61/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 2.3251e-04 - acc: 1.0000 - val_loss: 0.1734 - val_acc: 0.5312\n",
            "Epoch 62/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 2.2306e-04 - acc: 1.0000 - val_loss: 0.1738 - val_acc: 0.5312\n",
            "Epoch 63/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 2.3415e-04 - acc: 1.0000 - val_loss: 0.1732 - val_acc: 0.5156\n",
            "Epoch 64/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 2.3133e-04 - acc: 1.0000 - val_loss: 0.1734 - val_acc: 0.5156\n",
            "Epoch 65/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 2.2358e-04 - acc: 1.0000 - val_loss: 0.1743 - val_acc: 0.5312\n",
            "Epoch 66/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 2.1982e-04 - acc: 1.0000 - val_loss: 0.1742 - val_acc: 0.5312\n",
            "Epoch 67/128\n",
            "6/6 [==============================] - 1s 190ms/step - loss: 2.2388e-04 - acc: 1.0000 - val_loss: 0.1740 - val_acc: 0.5312\n",
            "Epoch 68/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 2.0053e-04 - acc: 1.0000 - val_loss: 0.1742 - val_acc: 0.5312\n",
            "Epoch 69/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 2.0527e-04 - acc: 1.0000 - val_loss: 0.1743 - val_acc: 0.5312\n",
            "Epoch 70/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.9164e-04 - acc: 1.0000 - val_loss: 0.1744 - val_acc: 0.5312\n",
            "Epoch 71/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 1.8313e-04 - acc: 1.0000 - val_loss: 0.1746 - val_acc: 0.5312\n",
            "Epoch 72/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.8528e-04 - acc: 1.0000 - val_loss: 0.1746 - val_acc: 0.5312\n",
            "Epoch 73/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.8543e-04 - acc: 1.0000 - val_loss: 0.1744 - val_acc: 0.5312\n",
            "Epoch 74/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.8344e-04 - acc: 1.0000 - val_loss: 0.1747 - val_acc: 0.5312\n",
            "Epoch 75/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.7617e-04 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 0.5312\n",
            "Epoch 76/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.5907e-04 - acc: 1.0000 - val_loss: 0.1746 - val_acc: 0.5156\n",
            "Epoch 77/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.4872e-04 - acc: 1.0000 - val_loss: 0.1745 - val_acc: 0.5156\n",
            "Epoch 78/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.6580e-04 - acc: 1.0000 - val_loss: 0.1747 - val_acc: 0.5156\n",
            "Epoch 79/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.5090e-04 - acc: 1.0000 - val_loss: 0.1753 - val_acc: 0.5312\n",
            "Epoch 80/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.4534e-04 - acc: 1.0000 - val_loss: 0.1750 - val_acc: 0.5312\n",
            "Epoch 81/128\n",
            "6/6 [==============================] - 1s 186ms/step - loss: 1.4512e-04 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 0.5156\n",
            "Epoch 82/128\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 1.5231e-04 - acc: 1.0000 - val_loss: 0.1751 - val_acc: 0.5312\n",
            "Epoch 83/128\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 1.4226e-04 - acc: 1.0000 - val_loss: 0.1752 - val_acc: 0.5156\n",
            "Epoch 84/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.4683e-04 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.5312\n",
            "Epoch 85/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.4002e-04 - acc: 1.0000 - val_loss: 0.1752 - val_acc: 0.5156\n",
            "Epoch 86/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 1.3048e-04 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.5312\n",
            "Epoch 87/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.3360e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5312\n",
            "Epoch 88/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.2784e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5312\n",
            "Epoch 89/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.3623e-04 - acc: 1.0000 - val_loss: 0.1755 - val_acc: 0.5312\n",
            "Epoch 90/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.3064e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5312\n",
            "Epoch 91/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.2951e-04 - acc: 1.0000 - val_loss: 0.1760 - val_acc: 0.5312\n",
            "Epoch 92/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.1469e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5156\n",
            "Epoch 93/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.1623e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5156\n",
            "Epoch 94/128\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 1.2486e-04 - acc: 1.0000 - val_loss: 0.1760 - val_acc: 0.5312\n",
            "Epoch 95/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.2295e-04 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.5156\n",
            "Epoch 96/128\n",
            "6/6 [==============================] - 1s 190ms/step - loss: 1.1561e-04 - acc: 1.0000 - val_loss: 0.1759 - val_acc: 0.5156\n",
            "Epoch 97/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.1909e-04 - acc: 1.0000 - val_loss: 0.1762 - val_acc: 0.5312\n",
            "Epoch 98/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.2462e-04 - acc: 1.0000 - val_loss: 0.1763 - val_acc: 0.5312\n",
            "Epoch 99/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 1.2056e-04 - acc: 1.0000 - val_loss: 0.1758 - val_acc: 0.5312\n",
            "Epoch 100/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 1.1188e-04 - acc: 1.0000 - val_loss: 0.1762 - val_acc: 0.5312\n",
            "Epoch 101/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.0361e-04 - acc: 1.0000 - val_loss: 0.1764 - val_acc: 0.5312\n",
            "Epoch 102/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 1.1257e-04 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.5312\n",
            "Epoch 103/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 1.0639e-04 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.5312\n",
            "Epoch 104/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.1486e-04 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.5312\n",
            "Epoch 105/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 1.0300e-04 - acc: 1.0000 - val_loss: 0.1765 - val_acc: 0.5312\n",
            "Epoch 106/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 1.1217e-04 - acc: 1.0000 - val_loss: 0.1766 - val_acc: 0.5312\n",
            "Epoch 107/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 9.9394e-05 - acc: 1.0000 - val_loss: 0.1764 - val_acc: 0.5312\n",
            "Epoch 108/128\n",
            "6/6 [==============================] - 1s 174ms/step - loss: 9.5023e-05 - acc: 1.0000 - val_loss: 0.1764 - val_acc: 0.5156\n",
            "Epoch 109/128\n",
            "6/6 [==============================] - 1s 172ms/step - loss: 9.6823e-05 - acc: 1.0000 - val_loss: 0.1765 - val_acc: 0.5469\n",
            "Epoch 110/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 9.2599e-05 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.5312\n",
            "Epoch 111/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 8.9944e-05 - acc: 1.0000 - val_loss: 0.1767 - val_acc: 0.5312\n",
            "Epoch 112/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 9.4480e-05 - acc: 1.0000 - val_loss: 0.1765 - val_acc: 0.5312\n",
            "Epoch 113/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 8.9676e-05 - acc: 1.0000 - val_loss: 0.1766 - val_acc: 0.5312\n",
            "Epoch 114/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 8.8547e-05 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.5312\n",
            "Epoch 115/128\n",
            "6/6 [==============================] - 1s 191ms/step - loss: 8.5223e-05 - acc: 1.0000 - val_loss: 0.1770 - val_acc: 0.5312\n",
            "Epoch 116/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 8.9658e-05 - acc: 1.0000 - val_loss: 0.1769 - val_acc: 0.5469\n",
            "Epoch 117/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 8.9965e-05 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.5312\n",
            "Epoch 118/128\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 8.2488e-05 - acc: 1.0000 - val_loss: 0.1769 - val_acc: 0.5469\n",
            "Epoch 119/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 8.7578e-05 - acc: 1.0000 - val_loss: 0.1769 - val_acc: 0.5312\n",
            "Epoch 120/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 9.3806e-05 - acc: 1.0000 - val_loss: 0.1770 - val_acc: 0.5312\n",
            "Epoch 121/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 8.7567e-05 - acc: 1.0000 - val_loss: 0.1769 - val_acc: 0.5312\n",
            "Epoch 122/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 8.3086e-05 - acc: 1.0000 - val_loss: 0.1772 - val_acc: 0.5469\n",
            "Epoch 123/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 8.1247e-05 - acc: 1.0000 - val_loss: 0.1770 - val_acc: 0.5312\n",
            "Epoch 124/128\n",
            "6/6 [==============================] - 1s 169ms/step - loss: 7.8666e-05 - acc: 1.0000 - val_loss: 0.1770 - val_acc: 0.5312\n",
            "Epoch 125/128\n",
            "6/6 [==============================] - 1s 171ms/step - loss: 8.6763e-05 - acc: 1.0000 - val_loss: 0.1771 - val_acc: 0.5312\n",
            "Epoch 126/128\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 7.7803e-05 - acc: 1.0000 - val_loss: 0.1771 - val_acc: 0.5312\n",
            "Epoch 127/128\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 8.4053e-05 - acc: 1.0000 - val_loss: 0.1772 - val_acc: 0.5312\n",
            "Epoch 128/128\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 7.7262e-05 - acc: 1.0000 - val_loss: 0.1771 - val_acc: 0.5312\n",
            "Complete.\n",
            "The accumulation time is 0:04:40.363312\n",
            "Last val_acc is 0.53125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCUgTvVNE2V1"
      },
      "source": [
        "## @学習可視化（history を一つだけ見る）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237Plt6pA82q"
      },
      "source": [
        "IO = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypG238EdgVWy"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "print(today)\n",
        "def plot_learning(history, experiment_name=\"No name\"):\n",
        "  HEIGHT = 1\n",
        "  WIDTH = 2\n",
        "  rate = 5.0\n",
        "  WpH_rate = 1.5\n",
        "  fig = plt.figure(figsize=(WIDTH*rate*WpH_rate, HEIGHT*rate))\n",
        "  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "  LOSS = fig.add_subplot(HEIGHT, WIDTH, 1) # loss, val_loss\n",
        "  ACC = fig.add_subplot(HEIGHT, WIDTH, 2) # acc, val_acc\n",
        "\n",
        "  # 1,1 loss\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  loss_props = {\n",
        "        'title' : 'Loss values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "  LOSS.set(**loss_props)\n",
        "  LOSS.plot(loss, label='loss', color='blue')\n",
        "  LOSS.plot(val_loss, label='val_loss', color='orange')\n",
        "  LOSS.legend(loc='best')\n",
        "\n",
        "  # 1,2 acc\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  acc_props = {\n",
        "        'title' : 'Accuracy values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "\n",
        "  ACC.set(**acc_props)\n",
        "  ACC.plot(acc, label='acc', color='blue')\n",
        "  ACC.plot(val_acc, label='val_acc', color='orange')\n",
        "  ACC.legend(loc='best')\n",
        "\n",
        "  #save\n",
        "  image_path = path.join(desk, experiment_name+today)\n",
        "  fig.savefig(image_path, bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YZZhYOBNQY"
      },
      "source": [
        "if IO:\n",
        "  plot_learning(histories[0], \"how_adam_used_for_KTH_in_certain_model\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57fZL4pkqsy"
      },
      "source": [
        "## @学習可視化データ保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGHTigrE-6F"
      },
      "source": [
        "if IO:\n",
        "  shelf = '/content/drive/MyDrive/colab'\n",
        "  book = 'tuning_results'\n",
        "  shelf_book = os.path.join(shelf, book)\n",
        "  shelf_book_page = path.join(shelf_book, today)\n",
        "  print(shelf_book_page)\n",
        "  # 保存\n",
        "  if not os.path.exists(shelf_book_page):\n",
        "    os.makedirs(shelf_book_page)\n",
        "\n",
        "  print(\"writing all to a strage now.\")\n",
        "  for each_file in os.listdir(desk):\n",
        "    if re.match(r\"\\..*\", each_file,):\n",
        "      pass\n",
        "    elif re.match(r\".*\\.png\", each_file,):\n",
        "      print(\"->\", each_file)\n",
        "      shutil.copy2(each_file, shelf_book_page)\n",
        "    else:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLC1pmB3FJQy"
      },
      "source": [
        "# 学習結果保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gjkqpp5bql"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "for key_loss, each_loss in enumerate(used_losses_set):\n",
        "  print(each_loss)\n",
        "  for key_experient_number, each_experiment in enumerate(all_histories):\n",
        "    print(\"   \", end=\"\")\n",
        "    print(\"experient_number :\", key_experient_number)\n",
        "    each_history = all_histories[key_experient_number][key_loss]\n",
        "    hist_df = pd.DataFrame(each_history.history)\n",
        "\n",
        "    name = \"KTH\" + \".\"\n",
        "    name = name + str(LABEL_NOISE_RATE) + \".\"\n",
        "    name = name + each_loss + \".\"\n",
        "    name = name + str(key_experient_number) + \".\"\n",
        "    name = name + today\n",
        "    name = name + \".csv\"\n",
        "\n",
        "    hist_df.to_csv(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9WGL2AX-G5b"
      },
      "source": [
        "print(\"all in the desk:\")\n",
        "files_in_the_desk = sorted(os.listdir(desk))\n",
        "for each in files_in_the_desk:\n",
        "  print(\"_____\", end=\"\")\n",
        "  print(each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7v3R0ur-xYD"
      },
      "source": [
        "directory_name = \"noize=\" + str(LABEL_NOISE_RATE)\n",
        "shelf_path = \"/content/drive/MyDrive/colab/CM_experiment/histories\"\n",
        "shelf_path = path.join(shelf_path, directory_name)\n",
        "\n",
        "if not path.exists(shelf_path):\n",
        "  os.makedirs(shelf_path)\n",
        "else: pass\n",
        "\n",
        "for each_file in files_in_the_desk:\n",
        "  shutil.copy2(each_file, shelf_path)\n",
        "\n",
        "\n",
        "massage = \"Histories are written\\n\"\n",
        "massage += \"Total time is \" + str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "\n",
        "send_line_notify(massage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}