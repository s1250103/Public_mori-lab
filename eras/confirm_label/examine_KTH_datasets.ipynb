{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_KTH_datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlEtO1Dx6nqqtfXVPU5sZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_KTH_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0\n",
        "TEST_DATA_RATE = 0.25"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : 1e-5,\n",
        "        \"momentum\" : 0,\n",
        "        \"decay\" : 0.1,\n",
        "        \"nesterov\" : False\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 4,\n",
        "        \"epochs\" : 1024,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                        #  tf.keras.losses.CategoricalCrossentropy(),\n",
        "                        #  tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  # dence\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  # 想定された入力CMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から計算される値"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "e9296258-b66d-4869-ee41-46d4cdaac01f"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/cleaned_detasets/KTH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "61e21548-bba9-4688-d5f4-def276847b4a"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"no such path\")\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/cleaned_detasets/KTH\n",
            "________|------------ reading [ Data_of_KTH.npz ] as learning data.\n",
            "________|------------ reading [ Label_of_KTH.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6rf5RcI7AzI",
        "outputId": "1a469350-1a72-4b8f-8454-c79b645f150d"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_test) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_test)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_test)\n",
        "set_max = np.max(Y_test)\n",
        "for i in range(len(Y_test)):\n",
        "  if i in change_points:\n",
        "    print(\"No.\", i, \", original number is\", Y_test[i], end=\" -> \")\n",
        "    Y_test[i] = changed_number(Y_test[i], set_min, set_max)\n",
        "    print(\"changed number is\", Y_test[i])\n",
        "  else:\n",
        "    print(\"error\")\n",
        "    break\n",
        "  "
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "change map: []\n",
            "the size: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiRg42hvLcYE"
      },
      "source": [
        "## 適切な形に処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ2Hpq_CLa1d"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sdOmjpeLke1",
        "outputId": "68a7526d-47d4-4600-de77-068b4f93ef7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = np.array(X_train).flatten()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKC7EMs-L7zk",
        "outputId": "452ce569-c6d6-4232-935b-48dd53d974e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-d2ba684acd0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnrNLfPhKq_J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXCHuDWpGxz-",
        "outputId": "bcdfbd88-5117-460d-f92a-12a78814d5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "obj_video_format = video_format()\n",
        "\n",
        "histories = []\n",
        "for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "  print(each_loss)\n",
        "  # print(re.search(r))\n",
        "\n",
        "  # モデル構築\n",
        "  model = make_model(obj_video_format)\n",
        "  model.compile(\n",
        "        optimizer=compile_optimizer(),\n",
        "        loss=each_loss,\n",
        "        metrics=['acc'])\n",
        "  # 実行\n",
        "  X_train, X_test, Y_train, Y_test\n",
        "\n",
        "  history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        validation_data=(X_test, Y_test),\n",
        "        # batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "        # epochs=10\n",
        "        # verbose=0\n",
        "        )\n",
        "  histories.append(history)\n",
        "  print(\"Complete.\")"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.losses.MeanAbsoluteError object at 0x7f86643e1128>\n",
            "sgd is used as a optimizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-ab943ea2cca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   history = model.fit(\n\u001b[1;32m     18\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# batch_size=learningDict[\"theWay\"][\"batch_size\"],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# epochs=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9laVNUpmLNey",
        "outputId": "c37fe038-48f6-4fa6-b676-a9ca369e1d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "  sleep(10)\n",
        "  del model\n",
        "  keras.backend.clear_session()\n",
        "  gc.collect()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kLrBLvsLB19",
        "outputId": "c24eacfa-f4d9-445c-889f-b286015cd22d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "learningDict[\"theWay\"][\"epochs\"]"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s08KXCe9HsfF",
        "outputId": "d83c4420-8730-43ca-a707-9a65aa5755a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "m = re.search(r'\\d+', s)\n",
        "\n",
        "print(m.group())"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.losses.MeanAbsoluteError at 0x7f8668403358>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    }
  ]
}