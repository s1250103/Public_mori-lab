{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_KTH_datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_KTH_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxGMERAvX3P8"
      },
      "source": [
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0.1\n",
        "TEST_DATA_RATE = 0.25\n",
        "EXPERIMENTS_NUMBER = 6"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : \"/notice! this is designed at before each learning\",\n",
        "        \"momentum\" : 0.9,\n",
        "        \"decay\" : 1e-4,\n",
        "        \"nesterov\" : True\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 128,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : 0.001,\n",
        "#         \"epsilon\" : 1e-8,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 128,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                         tf.keras.losses.MeanAbsoluteError(),\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        epsilon=learningDict[\"optimizer\"][\"epsilon\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  # 想定された入力CMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から計算される値"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "8177edcc-cc21-441e-a6bb-b06cd27ab73a"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already confirm\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/cleaned_detasets/KTH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "6b84816b-9157-4df2-e8d0-fde1e5312759"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"Not reading such data\", each_data)\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/cleaned_detasets/KTH\n",
            "Not reading such data .ipynb_checkpoints\n",
            "________|------------ reading [ Data_of_KTH.npz ] as learning data.\n",
            "________|------------ reading [ Label_of_KTH.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])\n",
        "\n",
        "learning_data = np.array(learning_data)\n",
        "label_data = np.array(label_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータの教師ラベルに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rf5RcI7AzI"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_train) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_train)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_train)\n",
        "set_max = np.max(Y_train)\n",
        "sum = 0\n",
        "if LABEL_NOISE_RATE != 0:\n",
        "  for i in range(len(Y_train)):\n",
        "    if i in change_points:\n",
        "      print(\"No.\", i, \", original number is\", Y_train[i], end=\" -> \")\n",
        "      Y_train[i] = changed_number(Y_train[i], set_min, set_max)\n",
        "      print(\"changed number is\", Y_train[i])\n",
        "      sum += 1\n",
        "    else:\n",
        "      pass\n",
        "else:\n",
        "  print(\"No label noizes\")\n",
        "  \n",
        "if sum == len(change_points):\n",
        "  print(\"changed correctlly.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM3OSjUWEPH3"
      },
      "source": [
        "##教師ラベルをone-hotに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHvnUv6KDty_"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(Y_train, 4)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, 4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3JzbydcYRv"
      },
      "source": [
        "def fix_loss_text(original):\n",
        "  result = re.sub(r\"<tensorflow\\.python\\.keras\\.losses\\.\", \"\", str(original))\n",
        "  result = re.sub(r\"\\sobject.+\", \"\", result)\n",
        "  return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQGLjvFcLRA"
      },
      "source": [
        "used_losses_set = []\n",
        "for each_loss in learningDict[\"compared_losses\"]:\n",
        "  used_losses_set.append(fix_loss_text(each_loss))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXCHuDWpGxz-",
        "outputId": "014b316d-48fe-493a-9aa6-3c15ce86e2b0"
      },
      "source": [
        "%%time\n",
        "obj_video_format = video_format()\n",
        "all_histories = [] # append EXPERIMENTS_NUMBER * used losses size\n",
        "\n",
        "start_time = time.time()\n",
        "for j in range(EXPERIMENTS_NUMBER):\n",
        "  seed_in_roop = seed + j\n",
        "  np.random.seed(seed_in_roop)\n",
        "  tf.random.set_seed(seed_in_roop)\n",
        "  print(\"runnng...\")\n",
        "\n",
        "  roop_histories = [] # append used losses size\n",
        "  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "\n",
        "    # 通知\n",
        "    all_loop_counter = str(j * len(learningDict[\"compared_losses\"]) + (i + 1))\n",
        "    all_loop_number = str(EXPERIMENTS_NUMBER * len(learningDict[\"compared_losses\"]))\n",
        "    \n",
        "    start_massage = \"Try the \" + all_loop_counter + \"/\" + all_loop_number + \" loop\\n\"\n",
        "    start_massage += \"Current used loss function is [\" + fix_loss_text(each_loss) + \"] = \" + str(i+1) +  \"/\" + str(len(learningDict[\"compared_losses\"]))\n",
        "\n",
        "    print(start_massage)\n",
        "    send_line_notify(start_massage)\n",
        "\n",
        "    try:\n",
        "      # モデル構築\n",
        "      model = make_model(obj_video_format)\n",
        "\n",
        "      if fix_loss_text(each_loss) == \"CategoricalCrossentropy\":\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-3)*2\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "      else:\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-2)\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "\n",
        "      model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=each_loss,\n",
        "            metrics=['acc'])\n",
        "      # 実行\n",
        "      \n",
        "      history = model.fit(\n",
        "            X_train, Y_train,\n",
        "            validation_data=(X_test, Y_test),\n",
        "            batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "            epochs=learningDict[\"theWay\"][\"epochs\"]\n",
        "            # verbose=0\n",
        "            )\n",
        "    except KeyboardInterrupt: \n",
        "      print(\"\\n\\nProcessing the KeyboardInterrupt\")\n",
        "\n",
        "    else:\n",
        "      roop_histories.append(history)\n",
        "      accumulation_time = str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "      finish_massage = \"Complete.\\n\"\n",
        "      finish_massage += \"The accumulation time is \" + str(accumulation_time) + \"\\n\"\n",
        "      finish_massage += \"Last val_acc is \" + str(history.history[\"val_acc\"][learningDict[\"theWay\"][\"epochs\"]-1])\n",
        "      print(finish_massage)\n",
        "      send_line_notify(finish_massage)\n",
        "    finally:\n",
        "      del model\n",
        "      keras.backend.clear_session()\n",
        "      gc.collect()\n",
        "      sleep(10)\n",
        "      clear_output()\n",
        "      print(\" and the model is erased.\")\n",
        "\n",
        "  #/for i\n",
        "  all_histories.append(roop_histories)\n",
        "\n",
        "#/for j\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " and the model is erased.\n",
            "Try the 2/18 loop\n",
            "Current used loss function is [MeanSquaredError] = 2/3\n",
            "sgd is used as a optimizer\n",
            "0.01\n",
            "Epoch 1/128\n",
            "10/10 [==============================] - 2s 166ms/step - loss: 0.1895 - acc: 0.2732 - val_loss: 0.1856 - val_acc: 0.2600\n",
            "Epoch 2/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1862 - acc: 0.3295 - val_loss: 0.1863 - val_acc: 0.3800\n",
            "Epoch 3/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.1821 - acc: 0.4723 - val_loss: 0.1820 - val_acc: 0.3800\n",
            "Epoch 4/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1761 - acc: 0.5092 - val_loss: 0.1773 - val_acc: 0.4000\n",
            "Epoch 5/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 0.1673 - acc: 0.5371 - val_loss: 0.1710 - val_acc: 0.3900\n",
            "Epoch 6/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 0.1565 - acc: 0.5972 - val_loss: 0.1667 - val_acc: 0.4100\n",
            "Epoch 7/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1437 - acc: 0.5922 - val_loss: 0.1582 - val_acc: 0.4700\n",
            "Epoch 8/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 0.1283 - acc: 0.5956 - val_loss: 0.1543 - val_acc: 0.5100\n",
            "Epoch 9/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1163 - acc: 0.7047 - val_loss: 0.1547 - val_acc: 0.4500\n",
            "Epoch 10/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1025 - acc: 0.7523 - val_loss: 0.1409 - val_acc: 0.5600\n",
            "Epoch 11/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0885 - acc: 0.8037 - val_loss: 0.1359 - val_acc: 0.5800\n",
            "Epoch 12/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0828 - acc: 0.8044 - val_loss: 0.1353 - val_acc: 0.5800\n",
            "Epoch 13/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 0.0675 - acc: 0.8412 - val_loss: 0.1359 - val_acc: 0.6200\n",
            "Epoch 14/128\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 0.0554 - acc: 0.8734 - val_loss: 0.1341 - val_acc: 0.6300\n",
            "Epoch 15/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0555 - acc: 0.8875 - val_loss: 0.1315 - val_acc: 0.6500\n",
            "Epoch 16/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0439 - acc: 0.9221 - val_loss: 0.1357 - val_acc: 0.6100\n",
            "Epoch 17/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0384 - acc: 0.9469 - val_loss: 0.1308 - val_acc: 0.6500\n",
            "Epoch 18/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0298 - acc: 0.9676 - val_loss: 0.1282 - val_acc: 0.6600\n",
            "Epoch 19/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0239 - acc: 0.9744 - val_loss: 0.1310 - val_acc: 0.6800\n",
            "Epoch 20/128\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 0.0262 - acc: 0.9583 - val_loss: 0.1431 - val_acc: 0.6100\n",
            "Epoch 21/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0296 - acc: 0.9425 - val_loss: 0.1338 - val_acc: 0.6300\n",
            "Epoch 22/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0218 - acc: 0.9688 - val_loss: 0.1302 - val_acc: 0.7000\n",
            "Epoch 23/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0151 - acc: 0.9856 - val_loss: 0.1318 - val_acc: 0.6500\n",
            "Epoch 24/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0163 - acc: 0.9748 - val_loss: 0.1336 - val_acc: 0.6600\n",
            "Epoch 25/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0118 - acc: 0.9806 - val_loss: 0.1315 - val_acc: 0.6600\n",
            "Epoch 26/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0146 - acc: 0.9736 - val_loss: 0.1348 - val_acc: 0.6100\n",
            "Epoch 27/128\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 0.0108 - acc: 0.9855 - val_loss: 0.1340 - val_acc: 0.6600\n",
            "Epoch 28/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0106 - acc: 0.9814 - val_loss: 0.1353 - val_acc: 0.6100\n",
            "Epoch 29/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0095 - acc: 0.9839 - val_loss: 0.1363 - val_acc: 0.6800\n",
            "Epoch 30/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0094 - acc: 0.9908 - val_loss: 0.1321 - val_acc: 0.6400\n",
            "Epoch 31/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0106 - acc: 0.9766 - val_loss: 0.1353 - val_acc: 0.6500\n",
            "Epoch 32/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0090 - acc: 0.9805 - val_loss: 0.1381 - val_acc: 0.6500\n",
            "Epoch 33/128\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 0.0081 - acc: 0.9893 - val_loss: 0.1435 - val_acc: 0.6200\n",
            "Epoch 34/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0157 - acc: 0.9536 - val_loss: 0.1383 - val_acc: 0.6500\n",
            "Epoch 35/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.0059 - acc: 0.9897 - val_loss: 0.1366 - val_acc: 0.6300\n",
            "Epoch 36/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0050 - acc: 0.9914 - val_loss: 0.1391 - val_acc: 0.6200\n",
            "Epoch 37/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0058 - acc: 0.9953 - val_loss: 0.1393 - val_acc: 0.6400\n",
            "Epoch 38/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0076 - acc: 0.9873 - val_loss: 0.1357 - val_acc: 0.6500\n",
            "Epoch 39/128\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 0.0051 - acc: 0.9852 - val_loss: 0.1360 - val_acc: 0.6600\n",
            "Epoch 40/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0056 - acc: 0.9856 - val_loss: 0.1377 - val_acc: 0.6400\n",
            "Epoch 41/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0058 - acc: 0.9856 - val_loss: 0.1371 - val_acc: 0.6700\n",
            "Epoch 42/128\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 0.0026 - acc: 0.9943 - val_loss: 0.1396 - val_acc: 0.6300\n",
            "Epoch 43/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 0.0022 - acc: 0.9983 - val_loss: 0.1381 - val_acc: 0.6600\n",
            "Epoch 44/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0017 - acc: 0.9991 - val_loss: 0.1388 - val_acc: 0.6500\n",
            "Epoch 45/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 0.0014 - acc: 0.9987 - val_loss: 0.1414 - val_acc: 0.6500\n",
            "Epoch 46/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1386 - val_acc: 0.6400\n",
            "Epoch 47/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1401 - val_acc: 0.6400\n",
            "Epoch 48/128\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0013 - acc: 1.0000\n",
            "\n",
            "Processing the KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a6415eab8a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'obj_video_format = video_format()\\nall_histories = [] # append EXPERIMENTS_NUMBER * used losses size\\n\\nstart_time = time.time()\\nfor j in range(EXPERIMENTS_NUMBER):\\n  seed_in_roop = seed + j\\n  np.random.seed(seed_in_roop)\\n  tf.random.set_seed(seed_in_roop)\\n  print(\"runnng...\")\\n\\n  roop_histories = [] # append used losses size\\n  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\\n\\n    # 通知\\n    all_loop_counter = str(j * len(learningDict[\"compared_losses\"]) + (i + 1))\\n    all_loop_number = str(EXPERIMENTS_NUMBER * len(learningDict[\"compared_losses\"]))\\n    \\n    start_massage = \"Try the \" + all_loop_counter + \"/\" + all_loop_number + \" loop\\\\n\"\\n    start_massage += \"Current used loss function is [\" + fix_loss_text(each_loss) + \"] = \" + str(i+1) +  \"/\" + str(len(learningDict[\"compared_losses\"]))\\n\\n    print(start_massage)\\n    send_line_notify(start_massage)\\n\\n    try:\\n      # モデル構築\\n      model = make_model(obj_video_format)\\n\\n      if fix_loss_text(each_loss) == \"CategoricalCrossentropy\":\\n        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-3)*2\\n        optimizer = compile_optimizer()\\n        print(learningDict[\"optimizer\"][\"learning_rate\"])\\n      else:\\n        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-2)\\n        optimizer = compile_optimizer()\\n        print...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCUgTvVNE2V1"
      },
      "source": [
        "## @学習可視化（history を一つだけ見る）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237Plt6pA82q"
      },
      "source": [
        "IO = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypG238EdgVWy"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "print(today)\n",
        "def plot_learning(history, experiment_name=\"No name\"):\n",
        "  HEIGHT = 1\n",
        "  WIDTH = 2\n",
        "  rate = 5.0\n",
        "  WpH_rate = 1.5\n",
        "  fig = plt.figure(figsize=(WIDTH*rate*WpH_rate, HEIGHT*rate))\n",
        "  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "  LOSS = fig.add_subplot(HEIGHT, WIDTH, 1) # loss, val_loss\n",
        "  ACC = fig.add_subplot(HEIGHT, WIDTH, 2) # acc, val_acc\n",
        "\n",
        "  # 1,1 loss\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  loss_props = {\n",
        "        'title' : 'Loss values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "  LOSS.set(**loss_props)\n",
        "  LOSS.plot(loss, label='loss', color='blue')\n",
        "  LOSS.plot(val_loss, label='val_loss', color='orange')\n",
        "  LOSS.legend(loc='best')\n",
        "\n",
        "  # 1,2 acc\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  acc_props = {\n",
        "        'title' : 'Accuracy values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "\n",
        "  ACC.set(**acc_props)\n",
        "  ACC.plot(acc, label='acc', color='blue')\n",
        "  ACC.plot(val_acc, label='val_acc', color='orange')\n",
        "  ACC.legend(loc='best')\n",
        "\n",
        "  #save\n",
        "  image_path = path.join(desk, experiment_name+today)\n",
        "  fig.savefig(image_path, bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YZZhYOBNQY"
      },
      "source": [
        "if IO:\n",
        "  plot_learning(histories[0], \"how_adam_used_for_KTH_in_certain_model\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57fZL4pkqsy"
      },
      "source": [
        "## @学習可視化データ保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGHTigrE-6F"
      },
      "source": [
        "if IO:\n",
        "  shelf = '/content/drive/MyDrive/colab'\n",
        "  book = 'tuning_results'\n",
        "  shelf_book = os.path.join(shelf, book)\n",
        "  shelf_book_page = path.join(shelf_book, today)\n",
        "  print(shelf_book_page)\n",
        "  # 保存\n",
        "  if not os.path.exists(shelf_book_page):\n",
        "    os.makedirs(shelf_book_page)\n",
        "\n",
        "  print(\"writing all to a strage now.\")\n",
        "  for each_file in os.listdir(desk):\n",
        "    if re.match(r\"\\..*\", each_file,):\n",
        "      pass\n",
        "    elif re.match(r\".*\\.png\", each_file,):\n",
        "      print(\"->\", each_file)\n",
        "      shutil.copy2(each_file, shelf_book_page)\n",
        "    else:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLC1pmB3FJQy"
      },
      "source": [
        "# 学習結果保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gjkqpp5bql"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "for key_loss, each_loss in enumerate(used_losses_set):\n",
        "  print(each_loss)\n",
        "  for key_experient_number, each_experiment in enumerate(all_histories):\n",
        "    print(\"   \", end=\"\")\n",
        "    print(\"experient_number :\", key_experient_number)\n",
        "    each_history = all_histories[key_experient_number][key_loss]\n",
        "    hist_df = pd.DataFrame(each_history.history)\n",
        "\n",
        "    name = \"KTH\" + \".\"\n",
        "    name = name + str(LABEL_NOISE_RATE) + \".\"\n",
        "    name = name + each_loss + \".\"\n",
        "    name = name + str(key_experient_number) + \".\"\n",
        "    name = name + today\n",
        "    name = name + \".csv\"\n",
        "\n",
        "    hist_df.to_csv(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9WGL2AX-G5b"
      },
      "source": [
        "print(\"all in the desk:\")\n",
        "files_in_the_desk = sorted(os.listdir(desk))\n",
        "for each in files_in_the_desk:\n",
        "  print(\"_____\", end=\"\")\n",
        "  print(each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7v3R0ur-xYD"
      },
      "source": [
        "directory_name = \"noize=\" + str(LABEL_NOISE_RATE)\n",
        "shelf_path = \"/content/drive/MyDrive/colab/histories/KTH_robust\" \n",
        "shelf_path = path.join(shelf_path, directory_name)\n",
        "\n",
        "if not path.exists(shelf_path):\n",
        "  os.makedirs(shelf_path)\n",
        "else: pass\n",
        "\n",
        "for each_file in files_in_the_desk:\n",
        "  shutil.copy2(each_file, shelf_path)\n",
        "\n",
        "\n",
        "massage = \"Histories are written\\n\"\n",
        "massage += \"Total time is \" + str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "\n",
        "send_line_notify(massage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}