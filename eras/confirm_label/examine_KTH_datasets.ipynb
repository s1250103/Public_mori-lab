{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_KTH_datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_KTH_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxGMERAvX3P8"
      },
      "source": [
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0.2\n",
        "TEST_DATA_RATE = 0.25\n",
        "EXPERIMENTS_NUMBER = 6"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : \"/notice! this is designed at before each learning\",\n",
        "        \"momentum\" : 0.9,\n",
        "        \"decay\" : 1e-4,\n",
        "        \"nesterov\" : True\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 128,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : 0.001,\n",
        "#         \"epsilon\" : 1e-8,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 128,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                         tf.keras.losses.MeanAbsoluteError(),\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        epsilon=learningDict[\"optimizer\"][\"epsilon\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  # 想定された入力CMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から計算される値"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "701489cc-71ed-429f-ce03-9341504f1d28"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/cleaned_detasets/KTH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "a60e3074-b95b-47d4-dc94-c86f0ad0cf7e"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"Not reading such data\", each_data)\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/cleaned_detasets/KTH\n",
            "Not reading such data .ipynb_checkpoints\n",
            "________|------------ reading [ Data_of_KTH.npz ] as learning data.\n",
            "________|------------ reading [ Label_of_KTH.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])\n",
        "\n",
        "learning_data = np.array(learning_data)\n",
        "label_data = np.array(label_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータの教師ラベルに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rf5RcI7AzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc111167-0de2-497f-f5b5-49acafa5cf52"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_train) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_train)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_train)\n",
        "set_max = np.max(Y_train)\n",
        "sum = 0\n",
        "if LABEL_NOISE_RATE != 0:\n",
        "  for i in range(len(Y_train)):\n",
        "    if i in change_points:\n",
        "      print(\"No.\", i, \", original number is\", Y_train[i], end=\" -> \")\n",
        "      Y_train[i] = changed_number(Y_train[i], set_min, set_max)\n",
        "      print(\"changed number is\", Y_train[i])\n",
        "      sum += 1\n",
        "    else:\n",
        "      pass\n",
        "else:\n",
        "  print(\"No label noizes\")\n",
        "  \n",
        "if sum == len(change_points):\n",
        "  print(\"changed correctlly.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "change map: [5, 10, 17, 23, 25, 30, 33, 38, 42, 45, 46, 51, 65, 75, 86, 88, 90, 94, 96, 117, 124, 125, 137, 141, 146, 150, 151, 158, 160, 175, 178, 179, 180, 182, 183, 193, 196, 203, 210, 211, 218, 220, 231, 235, 237, 238, 245, 246, 255, 257, 262, 264, 274, 276, 286, 287, 295, 296, 298]\n",
            "the size: 59\n",
            "No. 5 , original number is 2 -> changed number is 0\n",
            "No. 10 , original number is 1 -> changed number is 0\n",
            "No. 17 , original number is 0 -> changed number is 3\n",
            "No. 23 , original number is 3 -> changed number is 2\n",
            "No. 25 , original number is 1 -> changed number is 2\n",
            "No. 30 , original number is 3 -> changed number is 1\n",
            "No. 33 , original number is 3 -> changed number is 2\n",
            "No. 38 , original number is 1 -> changed number is 0\n",
            "No. 42 , original number is 0 -> changed number is 3\n",
            "No. 45 , original number is 1 -> changed number is 2\n",
            "No. 46 , original number is 3 -> changed number is 2\n",
            "No. 51 , original number is 3 -> changed number is 0\n",
            "No. 65 , original number is 1 -> changed number is 2\n",
            "No. 75 , original number is 3 -> changed number is 1\n",
            "No. 86 , original number is 0 -> changed number is 3\n",
            "No. 88 , original number is 0 -> changed number is 3\n",
            "No. 90 , original number is 1 -> changed number is 0\n",
            "No. 94 , original number is 0 -> changed number is 2\n",
            "No. 96 , original number is 0 -> changed number is 1\n",
            "No. 117 , original number is 1 -> changed number is 2\n",
            "No. 124 , original number is 2 -> changed number is 0\n",
            "No. 125 , original number is 2 -> changed number is 0\n",
            "No. 137 , original number is 3 -> changed number is 0\n",
            "No. 141 , original number is 2 -> changed number is 1\n",
            "No. 146 , original number is 1 -> changed number is 0\n",
            "No. 150 , original number is 2 -> changed number is 3\n",
            "No. 151 , original number is 3 -> changed number is 2\n",
            "No. 158 , original number is 3 -> changed number is 0\n",
            "No. 160 , original number is 1 -> changed number is 2\n",
            "No. 175 , original number is 0 -> changed number is 3\n",
            "No. 178 , original number is 1 -> changed number is 2\n",
            "No. 179 , original number is 3 -> changed number is 1\n",
            "No. 180 , original number is 1 -> changed number is 3\n",
            "No. 182 , original number is 1 -> changed number is 3\n",
            "No. 183 , original number is 1 -> changed number is 0\n",
            "No. 193 , original number is 3 -> changed number is 1\n",
            "No. 196 , original number is 2 -> changed number is 1\n",
            "No. 203 , original number is 0 -> changed number is 3\n",
            "No. 210 , original number is 0 -> changed number is 1\n",
            "No. 211 , original number is 0 -> changed number is 1\n",
            "No. 218 , original number is 0 -> changed number is 1\n",
            "No. 220 , original number is 1 -> changed number is 2\n",
            "No. 231 , original number is 3 -> changed number is 1\n",
            "No. 235 , original number is 1 -> changed number is 0\n",
            "No. 237 , original number is 0 -> changed number is 2\n",
            "No. 238 , original number is 1 -> changed number is 2\n",
            "No. 245 , original number is 2 -> changed number is 1\n",
            "No. 246 , original number is 0 -> changed number is 2\n",
            "No. 255 , original number is 1 -> changed number is 0\n",
            "No. 257 , original number is 3 -> changed number is 2\n",
            "No. 262 , original number is 1 -> changed number is 2\n",
            "No. 264 , original number is 1 -> changed number is 0\n",
            "No. 274 , original number is 0 -> changed number is 1\n",
            "No. 276 , original number is 2 -> changed number is 0\n",
            "No. 286 , original number is 2 -> changed number is 1\n",
            "No. 287 , original number is 3 -> changed number is 0\n",
            "No. 295 , original number is 0 -> changed number is 1\n",
            "No. 296 , original number is 3 -> changed number is 0\n",
            "No. 298 , original number is 1 -> changed number is 0\n",
            "changed correctlly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM3OSjUWEPH3"
      },
      "source": [
        "##教師ラベルをone-hotに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHvnUv6KDty_"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(Y_train, 4)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, 4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3JzbydcYRv"
      },
      "source": [
        "def fix_loss_text(original):\n",
        "  result = re.sub(r\"<tensorflow\\.python\\.keras\\.losses\\.\", \"\", str(original))\n",
        "  result = re.sub(r\"\\sobject.+\", \"\", result)\n",
        "  return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQGLjvFcLRA"
      },
      "source": [
        "used_losses_set = []\n",
        "for each_loss in learningDict[\"compared_losses\"]:\n",
        "  used_losses_set.append(fix_loss_text(each_loss))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXCHuDWpGxz-",
        "outputId": "551d6724-749a-4f98-9df3-867ece2cfd1a"
      },
      "source": [
        "%%time\n",
        "obj_video_format = video_format()\n",
        "all_histories = [] # append EXPERIMENTS_NUMBER * used losses size\n",
        "\n",
        "start_time = time.time()\n",
        "for j in range(EXPERIMENTS_NUMBER):\n",
        "  seed_in_roop = seed + j\n",
        "  np.random.seed(seed_in_roop)\n",
        "  tf.random.set_seed(seed_in_roop)\n",
        "  print(\"runnng...\")\n",
        "\n",
        "  roop_histories = [] # append used losses size\n",
        "  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "\n",
        "    # 通知\n",
        "    all_loop_counter = str(j * len(learningDict[\"compared_losses\"]) + (i + 1))\n",
        "    all_loop_number = str(EXPERIMENTS_NUMBER * len(learningDict[\"compared_losses\"]))\n",
        "    \n",
        "    start_massage = \"Try the \" + all_loop_counter + \"/\" + all_loop_number + \" loop\\n\"\n",
        "    start_massage += \"Current used loss function is [\" + fix_loss_text(each_loss) + \"] = \" + str(i+1) +  \"/\" + str(len(learningDict[\"compared_losses\"]))\n",
        "\n",
        "    print(start_massage)\n",
        "    send_line_notify(start_massage)\n",
        "\n",
        "    try:\n",
        "      # モデル構築\n",
        "      model = make_model(obj_video_format)\n",
        "\n",
        "      if fix_loss_text(each_loss) == \"CategoricalCrossentropy\":\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-3)*2\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "      else:\n",
        "        learningDict[\"optimizer\"][\"learning_rate\"] = (1e-2)\n",
        "        optimizer = compile_optimizer()\n",
        "        print(learningDict[\"optimizer\"][\"learning_rate\"])\n",
        "\n",
        "      model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=each_loss,\n",
        "            metrics=['acc'])\n",
        "      # 実行\n",
        "      \n",
        "      history = model.fit(\n",
        "            X_train, Y_train,\n",
        "            validation_data=(X_test, Y_test),\n",
        "            batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "            epochs=learningDict[\"theWay\"][\"epochs\"]\n",
        "            # verbose=0\n",
        "            )\n",
        "    except KeyboardInterrupt: \n",
        "      print(\"\\n\\nProcessing the KeyboardInterrupt\")\n",
        "\n",
        "    else:\n",
        "      roop_histories.append(history)\n",
        "      accumulation_time = str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "      finish_massage = \"Complete.\\n\"\n",
        "      finish_massage += \"The accumulation time is \" + str(accumulation_time) + \"\\n\"\n",
        "      finish_massage += \"Last val_acc is \" + str(history.history[\"val_acc\"][learningDict[\"theWay\"][\"epochs\"]-1])\n",
        "      print(finish_massage)\n",
        "      send_line_notify(finish_massage)\n",
        "    finally:\n",
        "      del model\n",
        "      keras.backend.clear_session()\n",
        "      gc.collect()\n",
        "      sleep(10)\n",
        "      clear_output()\n",
        "      print(\" and the model is erased.\")\n",
        "\n",
        "  #/for i\n",
        "  all_histories.append(roop_histories)\n",
        "\n",
        "#/for j\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " and the model is erased.\n",
            "CPU times: user 34min 26s, sys: 12min 12s, total: 46min 39s\n",
            "Wall time: 1h 3min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCUgTvVNE2V1"
      },
      "source": [
        "## @学習可視化（history を一つだけ見る）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237Plt6pA82q"
      },
      "source": [
        "IO = False"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypG238EdgVWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d825041e-d737-4594-dd65-108c0dd42c9d"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "print(today)\n",
        "def plot_learning(history, experiment_name=\"No name\"):\n",
        "  HEIGHT = 1\n",
        "  WIDTH = 2\n",
        "  rate = 5.0\n",
        "  WpH_rate = 1.5\n",
        "  fig = plt.figure(figsize=(WIDTH*rate*WpH_rate, HEIGHT*rate))\n",
        "  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "  LOSS = fig.add_subplot(HEIGHT, WIDTH, 1) # loss, val_loss\n",
        "  ACC = fig.add_subplot(HEIGHT, WIDTH, 2) # acc, val_acc\n",
        "\n",
        "  # 1,1 loss\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  loss_props = {\n",
        "        'title' : 'Loss values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "  LOSS.set(**loss_props)\n",
        "  LOSS.plot(loss, label='loss', color='blue')\n",
        "  LOSS.plot(val_loss, label='val_loss', color='orange')\n",
        "  LOSS.legend(loc='best')\n",
        "\n",
        "  # 1,2 acc\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  acc_props = {\n",
        "        'title' : 'Accuracy values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "\n",
        "  ACC.set(**acc_props)\n",
        "  ACC.plot(acc, label='acc', color='blue')\n",
        "  ACC.plot(val_acc, label='val_acc', color='orange')\n",
        "  ACC.legend(loc='best')\n",
        "\n",
        "  #save\n",
        "  image_path = path.join(desk, experiment_name+today)\n",
        "  fig.savefig(image_path, bbox_inches='tight')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YZZhYOBNQY"
      },
      "source": [
        "if IO:\n",
        "  plot_learning(histories[0], \"how_adam_used_for_KTH_in_certain_model\") "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57fZL4pkqsy"
      },
      "source": [
        "## @学習可視化データ保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGHTigrE-6F"
      },
      "source": [
        "if IO:\n",
        "  shelf = '/content/drive/MyDrive/colab'\n",
        "  book = 'tuning_results'\n",
        "  shelf_book = os.path.join(shelf, book)\n",
        "  shelf_book_page = path.join(shelf_book, today)\n",
        "  print(shelf_book_page)\n",
        "  # 保存\n",
        "  if not os.path.exists(shelf_book_page):\n",
        "    os.makedirs(shelf_book_page)\n",
        "\n",
        "  print(\"writing all to a strage now.\")\n",
        "  for each_file in os.listdir(desk):\n",
        "    if re.match(r\"\\..*\", each_file,):\n",
        "      pass\n",
        "    elif re.match(r\".*\\.png\", each_file,):\n",
        "      print(\"->\", each_file)\n",
        "      shutil.copy2(each_file, shelf_book_page)\n",
        "    else:\n",
        "      pass"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLC1pmB3FJQy"
      },
      "source": [
        "# 学習結果保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gjkqpp5bql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6182c06-c14d-4ab0-ac4d-8a0b53b47443"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "for key_loss, each_loss in enumerate(used_losses_set):\n",
        "  print(each_loss)\n",
        "  for key_experient_number, each_experiment in enumerate(all_histories):\n",
        "    print(\"   \", end=\"\")\n",
        "    print(\"experient_number :\", key_experient_number)\n",
        "    each_history = all_histories[key_experient_number][key_loss]\n",
        "    hist_df = pd.DataFrame(each_history.history)\n",
        "\n",
        "    name = \"KTH\" + \".\"\n",
        "    name = name + str(LABEL_NOISE_RATE) + \".\"\n",
        "    name = name + each_loss + \".\"\n",
        "    name = name + str(key_experient_number) + \".\"\n",
        "    name = name + today\n",
        "    name = name + \".csv\"\n",
        "\n",
        "    hist_df.to_csv(name)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CategoricalCrossentropy\n",
            "   experient_number : 0\n",
            "   experient_number : 1\n",
            "   experient_number : 2\n",
            "   experient_number : 3\n",
            "   experient_number : 4\n",
            "   experient_number : 5\n",
            "MeanSquaredError\n",
            "   experient_number : 0\n",
            "   experient_number : 1\n",
            "   experient_number : 2\n",
            "   experient_number : 3\n",
            "   experient_number : 4\n",
            "   experient_number : 5\n",
            "MeanAbsoluteError\n",
            "   experient_number : 0\n",
            "   experient_number : 1\n",
            "   experient_number : 2\n",
            "   experient_number : 3\n",
            "   experient_number : 4\n",
            "   experient_number : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9WGL2AX-G5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584ce78d-033c-471a-b542-076e81c03bbe"
      },
      "source": [
        "print(\"all in the desk:\")\n",
        "files_in_the_desk = sorted(os.listdir(desk))\n",
        "for each in files_in_the_desk:\n",
        "  print(\"_____\", end=\"\")\n",
        "  print(each)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all in the desk:\n",
            "_____KTH.0.2.CategoricalCrossentropy.0.0131.csv\n",
            "_____KTH.0.2.CategoricalCrossentropy.1.0131.csv\n",
            "_____KTH.0.2.CategoricalCrossentropy.2.0131.csv\n",
            "_____KTH.0.2.CategoricalCrossentropy.3.0131.csv\n",
            "_____KTH.0.2.CategoricalCrossentropy.4.0131.csv\n",
            "_____KTH.0.2.CategoricalCrossentropy.5.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.0.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.1.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.2.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.3.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.4.0131.csv\n",
            "_____KTH.0.2.MeanAbsoluteError.5.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.0.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.1.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.2.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.3.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.4.0131.csv\n",
            "_____KTH.0.2.MeanSquaredError.5.0131.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7v3R0ur-xYD"
      },
      "source": [
        "directory_name = \"noize=\" + str(LABEL_NOISE_RATE)\n",
        "shelf_path = \"/content/drive/MyDrive/colab/histories/KTH_robust\" \n",
        "shelf_path = path.join(shelf_path, directory_name)\n",
        "\n",
        "if not path.exists(shelf_path):\n",
        "  os.makedirs(shelf_path)\n",
        "else: pass\n",
        "\n",
        "for each_file in files_in_the_desk:\n",
        "  shutil.copy2(each_file, shelf_path)\n",
        "\n",
        "\n",
        "massage = \"Histories are written\\n\"\n",
        "massage += \"Total time is \" + str(datetime.timedelta(seconds=(time.time() - start_time)))\n",
        "\n",
        "send_line_notify(massage)"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}