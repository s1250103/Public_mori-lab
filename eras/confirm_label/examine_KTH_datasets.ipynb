{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_KTH_datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGhPlGRD/mXq4oy5nbD0gg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_KTH_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxGMERAvX3P8"
      },
      "source": [
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0\n",
        "TEST_DATA_RATE = 0.25\n",
        "EXPERIMENTS_NUMBER = 6"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : 0.01,\n",
        "        \"momentum\" : 0.9,\n",
        "        \"decay\" : 1e-4,\n",
        "        \"nesterov\" : True\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 128,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : 0.001,\n",
        "#         \"epsilon\" : 1e-8,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 128,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                         tf.keras.losses.MeanAbsoluteError(),\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        epsilon=learningDict[\"optimizer\"][\"epsilon\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  # 想定された入力CMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から計算される値"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "1b043109-9138-4c7e-f225-0fd378233478"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/cleaned_detasets/KTH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "bfd1dae1-fd8b-47f8-80cc-f3f5c564c7e8"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"Not reading such data\", each_data)\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/cleaned_detasets/KTH\n",
            "Not reading such data .ipynb_checkpoints\n",
            "________|------------ reading [ Data_of_KTH.npz ] as learning data.\n",
            "________|------------ reading [ Label_of_KTH.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])\n",
        "\n",
        "learning_data = np.array(learning_data)\n",
        "label_data = np.array(label_data)\n",
        "label_data = tf.keras.utils.to_categorical(label_data, 4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6rf5RcI7AzI",
        "outputId": "344a12e4-1d6a-401b-b661-c3f60ad47eb4"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_test) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_test)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_test)\n",
        "set_max = np.max(Y_test)\n",
        "if LABEL_NOISE_RATE != 0:\n",
        "  for i in range(len(Y_test)):\n",
        "    if i in change_points:\n",
        "      print(\"No.\", i, \", original number is\", Y_test[i], end=\" -> \")\n",
        "      Y_test[i] = changed_number(Y_test[i], set_min, set_max)\n",
        "      print(\"changed number is\", Y_test[i])\n",
        "    else:\n",
        "      print(\"error\")\n",
        "      break\n",
        "else:\n",
        "  print(\"No label noizes\")\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "change map: []\n",
            "the size: 0\n",
            "No label noizes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrxLWQdNZrpU",
        "outputId": "6cf45548-71d5-4036-824e-c3b949428597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3JzbydcYRv"
      },
      "source": [
        "def fix_loss_text(original):\n",
        "  result = re.sub(r\"<tensorflow\\.python\\.keras\\.losses\\.\", \"\", str(original))\n",
        "  result = re.sub(r\"\\sobject.+\", \"\", result)\n",
        "  return result\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQGLjvFcLRA"
      },
      "source": [
        "used_losses_set = []\n",
        "for each_loss in learningDict[\"compared_losses\"]:\n",
        "  used_losses_set.append(fix_loss_text(each_loss))\n",
        "  \n",
        "    "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlTJnek9iJwN",
        "outputId": "4b7bf998-9432-4729-d95b-630791abdf45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CategoricalCrossentropy', 'MeanSquaredError', 'MeanAbsoluteError']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXCHuDWpGxz-",
        "outputId": "2747dcb2-9054-4aaa-8e8e-c28c3956b037"
      },
      "source": [
        "%%time\n",
        "obj_video_format = video_format()\n",
        "all_histories = [] # append EXPERIMENTS_NUMBER * used losses size\n",
        "for j in range(EXPERIMENTS_NUMBER):\n",
        "  seed_in_roop = seed + j\n",
        "  np.random.seed(seed_in_roop)\n",
        "  tf.random.set_seed(seed_in_roop)\n",
        "  massage = \"Start experiment in the No.\" + str(j+1) + \" loop.\"\n",
        "  print(massage)\n",
        "  print(\"also assigned[\", seed_in_roop, \"]as the seed. (the format is integer).\")\n",
        "  print(\"used loss functions\", len(used_losses_set), \"is :\", used_losses_set)\n",
        "  print(\"then notifying to certain line account\")\n",
        "  send_line_notify(massage)\n",
        "  print(\"runnng...\")\n",
        "\n",
        "  roop_histories = [] # append used losses size\n",
        "  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "    massage = \"try \" + fix_loss_text(each_loss) + \"the No.[\" + str(i+1) +\"]\"\n",
        "    print(massage)\n",
        "    send_line_notify(massage)\n",
        "\n",
        "    try:\n",
        "      # モデル構築\n",
        "      model = make_model(obj_video_format)\n",
        "      model.compile(\n",
        "            optimizer=compile_optimizer(),\n",
        "            loss=each_loss,\n",
        "            metrics=['acc'])\n",
        "      # 実行\n",
        "      \n",
        "      history = model.fit(\n",
        "            X_train, Y_train,\n",
        "            validation_data=(X_test, Y_test),\n",
        "            batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "            epochs=learningDict[\"theWay\"][\"epochs\"]\n",
        "            # verbose=0\n",
        "            )\n",
        "    except KeyboardInterrupt: \n",
        "      print(\"\\n\\nProcessing the KeyboardInterrupt\")\n",
        "\n",
        "    else:\n",
        "      roop_histories.append(history)\n",
        "      massage = \"Complete \" + fix_loss_text(each_loss) + \" the No.[\" + str(i+1) +\"]\"\n",
        "      print(massage)\n",
        "      send_line_notify(massage)\n",
        "    finally:\n",
        "      del model\n",
        "      keras.backend.clear_session()\n",
        "      gc.collect()\n",
        "      sleep(10)\n",
        "      print(\" and the model is erased.\")\n",
        "\n",
        "  #/for i\n",
        "  all_histories.append(roop_histories)\n",
        "  massage = \"Finish experiment in the No.\" + str(j+1) + \" loop.\"\n",
        "  print(massage)\n",
        "  print(\"then notifying to certain line account\")\n",
        "  send_line_notify(massage)\n",
        "\n",
        "#/for j\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start experiment in the No.1 loop.\n",
            "also assigned[ 20201218 ]as the seed. (the format is integer).\n",
            "used loss functions 3 is : ['CategoricalCrossentropy', 'MeanSquaredError', 'MeanAbsoluteError']\n",
            "then notifying to certain line account\n",
            "runnng...\n",
            "tryCategoricalCrossentropythe No.[1]\n",
            "sgd is used as a optimizer\n",
            "Epoch 1/128\n",
            "10/10 [==============================] - 9s 200ms/step - loss: 1.6303 - acc: 0.2265 - val_loss: 1.3837 - val_acc: 0.3200\n",
            "Epoch 2/128\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.3872 - acc: 0.1883 - val_loss: 1.3854 - val_acc: 0.2200\n",
            "Epoch 3/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 1.3828 - acc: 0.2549 - val_loss: 1.3836 - val_acc: 0.2700\n",
            "Epoch 4/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3753 - acc: 0.3802 - val_loss: 1.4137 - val_acc: 0.2200\n",
            "Epoch 5/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3630 - acc: 0.2943 - val_loss: 1.3885 - val_acc: 0.2500\n",
            "Epoch 6/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3846 - acc: 0.2703 - val_loss: 1.3882 - val_acc: 0.2200\n",
            "Epoch 7/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3849 - acc: 0.2875 - val_loss: 1.3887 - val_acc: 0.2700\n",
            "Epoch 8/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 1.3845 - acc: 0.3642 - val_loss: 1.3871 - val_acc: 0.2200\n",
            "Epoch 9/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3839 - acc: 0.2531 - val_loss: 1.3876 - val_acc: 0.2300\n",
            "Epoch 10/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 1.3782 - acc: 0.3179 - val_loss: 1.3728 - val_acc: 0.3800\n",
            "Epoch 11/128\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.3601 - acc: 0.4191 - val_loss: 1.3771 - val_acc: 0.3100\n",
            "Epoch 12/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.3781 - acc: 0.3151 - val_loss: 1.3716 - val_acc: 0.4300\n",
            "Epoch 13/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.3683 - acc: 0.3714 - val_loss: 1.3670 - val_acc: 0.2500\n",
            "Epoch 14/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3343 - acc: 0.3377 - val_loss: 1.1804 - val_acc: 0.4500\n",
            "Epoch 15/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.1073 - acc: 0.4954 - val_loss: 1.0855 - val_acc: 0.4700\n",
            "Epoch 16/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.8660 - acc: 0.5973 - val_loss: 0.8008 - val_acc: 0.6000\n",
            "Epoch 17/128\n",
            "10/10 [==============================] - 2s 159ms/step - loss: 0.7363 - acc: 0.6092 - val_loss: 0.9181 - val_acc: 0.5700\n",
            "Epoch 18/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.7976 - acc: 0.6798 - val_loss: 0.9426 - val_acc: 0.4700\n",
            "Epoch 19/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.6373 - acc: 0.6732 - val_loss: 1.2927 - val_acc: 0.5200\n",
            "Epoch 20/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.6462 - acc: 0.6319 - val_loss: 0.8386 - val_acc: 0.5900\n",
            "Epoch 21/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.5498 - acc: 0.7368 - val_loss: 0.6646 - val_acc: 0.5900\n",
            "Epoch 22/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.4660 - acc: 0.7752 - val_loss: 1.0644 - val_acc: 0.5000\n",
            "Epoch 23/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 1.0675 - acc: 0.4805 - val_loss: 0.9774 - val_acc: 0.4900\n",
            "Epoch 24/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.7767 - acc: 0.6719 - val_loss: 0.8604 - val_acc: 0.5500\n",
            "Epoch 25/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.6021 - acc: 0.7404 - val_loss: 0.8262 - val_acc: 0.5800\n",
            "Epoch 26/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.4243 - acc: 0.7705 - val_loss: 0.6995 - val_acc: 0.6400\n",
            "Epoch 27/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.3781 - acc: 0.8365 - val_loss: 0.7311 - val_acc: 0.6600\n",
            "Epoch 28/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.3691 - acc: 0.8386 - val_loss: 0.7293 - val_acc: 0.6600\n",
            "Epoch 29/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.3135 - acc: 0.8341 - val_loss: 1.0606 - val_acc: 0.6200\n",
            "Epoch 30/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.2571 - acc: 0.8719 - val_loss: 0.7467 - val_acc: 0.6700\n",
            "Epoch 31/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.2110 - acc: 0.9320 - val_loss: 0.7909 - val_acc: 0.6900\n",
            "Epoch 32/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.1557 - acc: 0.9539 - val_loss: 0.8704 - val_acc: 0.6700\n",
            "Epoch 33/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.1329 - acc: 0.9385 - val_loss: 0.9492 - val_acc: 0.6300\n",
            "Epoch 34/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.1047 - acc: 0.9595 - val_loss: 0.8934 - val_acc: 0.6800\n",
            "Epoch 35/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0739 - acc: 0.9830 - val_loss: 1.2830 - val_acc: 0.6800\n",
            "Epoch 36/128\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.0877 - acc: 0.9639 - val_loss: 0.8529 - val_acc: 0.7100\n",
            "Epoch 37/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0853 - acc: 0.9557 - val_loss: 1.4697 - val_acc: 0.6500\n",
            "Epoch 38/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.1068 - acc: 0.9443 - val_loss: 1.1899 - val_acc: 0.6900\n",
            "Epoch 39/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0360 - acc: 0.9930 - val_loss: 1.8655 - val_acc: 0.6400\n",
            "Epoch 40/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.0412 - acc: 0.9841 - val_loss: 1.2230 - val_acc: 0.6900\n",
            "Epoch 41/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0605 - acc: 0.9839 - val_loss: 1.2907 - val_acc: 0.6800\n",
            "Epoch 42/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.0786 - acc: 0.9740 - val_loss: 1.2110 - val_acc: 0.6600\n",
            "Epoch 43/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.3263 - acc: 0.9045 - val_loss: 1.4066 - val_acc: 0.6400\n",
            "Epoch 44/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.1720 - acc: 0.9453 - val_loss: 1.6339 - val_acc: 0.6500\n",
            "Epoch 45/128\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.0438 - acc: 0.9847 - val_loss: 1.3729 - val_acc: 0.7100\n",
            "Epoch 46/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0300 - acc: 0.9956 - val_loss: 1.4701 - val_acc: 0.6900\n",
            "Epoch 47/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.0229 - acc: 0.9940 - val_loss: 1.4242 - val_acc: 0.6800\n",
            "Epoch 48/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.0325 - acc: 0.9914 - val_loss: 1.6859 - val_acc: 0.6800\n",
            "Epoch 49/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 1.6819 - val_acc: 0.6900\n",
            "Epoch 50/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 1.4287 - val_acc: 0.7100\n",
            "Epoch 51/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 1.5701 - val_acc: 0.7100\n",
            "Epoch 52/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 1.6023 - val_acc: 0.7000\n",
            "Epoch 53/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.6626 - val_acc: 0.7100\n",
            "Epoch 54/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.6438 - val_acc: 0.7200\n",
            "Epoch 55/128\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.6325 - val_acc: 0.7000\n",
            "Epoch 56/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.6161 - val_acc: 0.7000\n",
            "Epoch 57/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.6714 - val_acc: 0.7000\n",
            "Epoch 58/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.6541 - val_acc: 0.6900\n",
            "Epoch 59/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.6758 - val_acc: 0.6900\n",
            "Epoch 60/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.6850 - val_acc: 0.7000\n",
            "Epoch 61/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 8.7710e-04 - acc: 1.0000 - val_loss: 1.6836 - val_acc: 0.6900\n",
            "Epoch 62/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 8.4454e-04 - acc: 1.0000 - val_loss: 1.6898 - val_acc: 0.6900\n",
            "Epoch 63/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.7096 - val_acc: 0.7100\n",
            "Epoch 64/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 8.9656e-04 - acc: 1.0000 - val_loss: 1.7192 - val_acc: 0.7200\n",
            "Epoch 65/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 8.9727e-04 - acc: 1.0000 - val_loss: 1.7184 - val_acc: 0.7000\n",
            "Epoch 66/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 9.1951e-04 - acc: 1.0000 - val_loss: 1.7254 - val_acc: 0.6900\n",
            "Epoch 67/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 7.1711e-04 - acc: 1.0000 - val_loss: 1.7303 - val_acc: 0.7000\n",
            "Epoch 68/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 7.0030e-04 - acc: 1.0000 - val_loss: 1.7354 - val_acc: 0.6900\n",
            "Epoch 69/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 7.5797e-04 - acc: 1.0000 - val_loss: 1.7645 - val_acc: 0.6900\n",
            "Epoch 70/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 7.2224e-04 - acc: 1.0000 - val_loss: 1.7669 - val_acc: 0.6900\n",
            "Epoch 71/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 6.5214e-04 - acc: 1.0000 - val_loss: 1.7560 - val_acc: 0.6900\n",
            "Epoch 72/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 6.5734e-04 - acc: 1.0000 - val_loss: 1.7638 - val_acc: 0.6900\n",
            "Epoch 73/128\n",
            "10/10 [==============================] - 2s 160ms/step - loss: 5.2411e-04 - acc: 1.0000 - val_loss: 1.7787 - val_acc: 0.6900\n",
            "Epoch 74/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 6.0971e-04 - acc: 1.0000 - val_loss: 1.7762 - val_acc: 0.7000\n",
            "Epoch 75/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 5.7027e-04 - acc: 1.0000 - val_loss: 1.7820 - val_acc: 0.6900\n",
            "Epoch 76/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 6.1052e-04 - acc: 1.0000 - val_loss: 1.7955 - val_acc: 0.6900\n",
            "Epoch 77/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 5.3789e-04 - acc: 1.0000 - val_loss: 1.7902 - val_acc: 0.6900\n",
            "Epoch 78/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 3.9948e-04 - acc: 1.0000 - val_loss: 1.7921 - val_acc: 0.6900\n",
            "Epoch 79/128\n",
            "10/10 [==============================] - 2s 164ms/step - loss: 4.8624e-04 - acc: 1.0000 - val_loss: 1.7875 - val_acc: 0.6900\n",
            "Epoch 80/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 3.8712e-04 - acc: 1.0000 - val_loss: 1.7959 - val_acc: 0.6900\n",
            "Epoch 81/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 3.3521e-04 - acc: 1.0000 - val_loss: 1.8016 - val_acc: 0.6900\n",
            "Epoch 82/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 4.6114e-04 - acc: 1.0000 - val_loss: 1.8100 - val_acc: 0.6900\n",
            "Epoch 83/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 3.8361e-04 - acc: 1.0000 - val_loss: 1.8227 - val_acc: 0.6900\n",
            "Epoch 84/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 5.0404e-04 - acc: 1.0000 - val_loss: 1.8209 - val_acc: 0.6900\n",
            "Epoch 85/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 4.5121e-04 - acc: 1.0000 - val_loss: 1.8202 - val_acc: 0.6900\n",
            "Epoch 86/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 3.2537e-04 - acc: 1.0000 - val_loss: 1.8242 - val_acc: 0.6900\n",
            "Epoch 87/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 3.7018e-04 - acc: 1.0000 - val_loss: 1.8274 - val_acc: 0.6900\n",
            "Epoch 88/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 4.0439e-04 - acc: 1.0000 - val_loss: 1.8290 - val_acc: 0.6900\n",
            "Epoch 89/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 3.7525e-04 - acc: 1.0000 - val_loss: 1.8355 - val_acc: 0.6900\n",
            "Epoch 90/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 3.1623e-04 - acc: 1.0000 - val_loss: 1.8493 - val_acc: 0.6800\n",
            "Epoch 91/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 4.9373e-04 - acc: 1.0000 - val_loss: 1.8487 - val_acc: 0.6900\n",
            "Epoch 92/128\n",
            "10/10 [==============================] - 2s 163ms/step - loss: 2.8792e-04 - acc: 1.0000 - val_loss: 1.8480 - val_acc: 0.6900\n",
            "Epoch 93/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 3.2644e-04 - acc: 1.0000 - val_loss: 1.8478 - val_acc: 0.6900\n",
            "Epoch 94/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 3.1620e-04 - acc: 1.0000 - val_loss: 1.8477 - val_acc: 0.6900\n",
            "Epoch 95/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 2.9434e-04 - acc: 1.0000 - val_loss: 1.8520 - val_acc: 0.6900\n",
            "Epoch 96/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 3.1115e-04 - acc: 1.0000 - val_loss: 1.8512 - val_acc: 0.6900\n",
            "Epoch 97/128\n",
            "10/10 [==============================] - 2s 157ms/step - loss: 2.8745e-04 - acc: 1.0000 - val_loss: 1.8534 - val_acc: 0.6900\n",
            "Epoch 98/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 2.9538e-04 - acc: 1.0000 - val_loss: 1.8588 - val_acc: 0.6900\n",
            "Epoch 99/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 3.0150e-04 - acc: 1.0000 - val_loss: 1.8596 - val_acc: 0.6900\n",
            "Epoch 100/128\n",
            "10/10 [==============================] - 1s 153ms/step - loss: 3.7515e-04 - acc: 1.0000 - val_loss: 1.8616 - val_acc: 0.6900\n",
            "Epoch 101/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 3.0639e-04 - acc: 1.0000 - val_loss: 1.8670 - val_acc: 0.6900\n",
            "Epoch 102/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 2.7480e-04 - acc: 1.0000 - val_loss: 1.8716 - val_acc: 0.6900\n",
            "Epoch 103/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 2.7824e-04 - acc: 1.0000 - val_loss: 1.8700 - val_acc: 0.6900\n",
            "Epoch 104/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 3.2584e-04 - acc: 1.0000 - val_loss: 1.8686 - val_acc: 0.6900\n",
            "Epoch 105/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 2.1803e-04 - acc: 1.0000 - val_loss: 1.8707 - val_acc: 0.6900\n",
            "Epoch 106/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.7136e-04 - acc: 1.0000 - val_loss: 1.8772 - val_acc: 0.6900\n",
            "Epoch 107/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.2520e-04 - acc: 1.0000 - val_loss: 1.8783 - val_acc: 0.7000\n",
            "Epoch 108/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 2.3973e-04 - acc: 1.0000 - val_loss: 1.8784 - val_acc: 0.7000\n",
            "Epoch 109/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.3559e-04 - acc: 1.0000 - val_loss: 1.8825 - val_acc: 0.7000\n",
            "Epoch 110/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 2.5257e-04 - acc: 1.0000 - val_loss: 1.8838 - val_acc: 0.7000\n",
            "Epoch 111/128\n",
            "10/10 [==============================] - 2s 162ms/step - loss: 2.3780e-04 - acc: 1.0000 - val_loss: 1.8888 - val_acc: 0.7000\n",
            "Epoch 112/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 2.1813e-04 - acc: 1.0000 - val_loss: 1.8962 - val_acc: 0.7000\n",
            "Epoch 113/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 2.3597e-04 - acc: 1.0000 - val_loss: 1.8950 - val_acc: 0.7000\n",
            "Epoch 114/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 1.9943e-04 - acc: 1.0000 - val_loss: 1.8982 - val_acc: 0.7000\n",
            "Epoch 115/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.4061e-04 - acc: 1.0000 - val_loss: 1.9004 - val_acc: 0.7000\n",
            "Epoch 116/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 2.1982e-04 - acc: 1.0000 - val_loss: 1.9044 - val_acc: 0.7000\n",
            "Epoch 117/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.1093e-04 - acc: 1.0000 - val_loss: 1.9095 - val_acc: 0.6900\n",
            "Epoch 118/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 2.0205e-04 - acc: 1.0000 - val_loss: 1.9094 - val_acc: 0.6900\n",
            "Epoch 119/128\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 2.1157e-04 - acc: 1.0000 - val_loss: 1.9099 - val_acc: 0.7000\n",
            "Epoch 120/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.1023e-04 - acc: 1.0000 - val_loss: 1.9089 - val_acc: 0.7000\n",
            "Epoch 121/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 2.5867e-04 - acc: 1.0000 - val_loss: 1.9064 - val_acc: 0.7000\n",
            "Epoch 122/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 2.0097e-04 - acc: 1.0000 - val_loss: 1.9077 - val_acc: 0.7000\n",
            "Epoch 123/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 2.3357e-04 - acc: 1.0000 - val_loss: 1.9090 - val_acc: 0.7000\n",
            "Epoch 124/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 1.5947e-04 - acc: 1.0000 - val_loss: 1.9112 - val_acc: 0.7000\n",
            "Epoch 125/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 1.5722e-04 - acc: 1.0000 - val_loss: 1.9151 - val_acc: 0.7000\n",
            "Epoch 126/128\n",
            "10/10 [==============================] - 2s 154ms/step - loss: 2.0376e-04 - acc: 1.0000 - val_loss: 1.9222 - val_acc: 0.7000\n",
            "Epoch 127/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 1.8572e-04 - acc: 1.0000 - val_loss: 1.9215 - val_acc: 0.7000\n",
            "Epoch 128/128\n",
            "10/10 [==============================] - 1s 151ms/step - loss: 1.6909e-04 - acc: 1.0000 - val_loss: 1.9200 - val_acc: 0.7000\n",
            "CompleteCategoricalCrossentropythe No.[1]\n",
            " and the model is erased.\n",
            "tryMeanSquaredErrorthe No.[2]\n",
            "sgd is used as a optimizer\n",
            "Epoch 1/128\n",
            "10/10 [==============================] - 2s 169ms/step - loss: 0.1895 - acc: 0.2732 - val_loss: 0.1859 - val_acc: 0.2400\n",
            "Epoch 2/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.1862 - acc: 0.3295 - val_loss: 0.1860 - val_acc: 0.4000\n",
            "Epoch 3/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1821 - acc: 0.4723 - val_loss: 0.1806 - val_acc: 0.4200\n",
            "Epoch 4/128\n",
            "10/10 [==============================] - 1s 152ms/step - loss: 0.1761 - acc: 0.5034 - val_loss: 0.1740 - val_acc: 0.4500\n",
            "Epoch 5/128\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.1672 - acc: 0.5405 - val_loss: 0.1657 - val_acc: 0.4400\n",
            "Epoch 6/128\n",
            "10/10 [==============================] - 2s 155ms/step - loss: 0.1565 - acc: 0.5910 - val_loss: 0.1564 - val_acc: 0.4700\n",
            "Epoch 7/128\n",
            "10/10 [==============================] - 2s 152ms/step - loss: 0.1436 - acc: 0.6048 - val_loss: 0.1436 - val_acc: 0.5300\n",
            "Epoch 8/128\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 0.1284 - acc: 0.5953 - val_loss: 0.1355 - val_acc: 0.5800\n",
            "Epoch 9/128\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 0.1161 - acc: 0.7073 - val_loss: 0.1338 - val_acc: 0.5300\n",
            "Epoch 10/128\n",
            "10/10 [==============================] - 2s 156ms/step - loss: 0.1028 - acc: 0.7477 - val_loss: 0.1162 - val_acc: 0.6500\n",
            "Epoch 11/128\n",
            " 1/10 [==>...........................] - ETA: 1s - loss: 0.0962 - acc: 0.8438"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKELx5csgTNC"
      },
      "source": [
        "# 学習可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypG238EdgVWy",
        "outputId": "579a02c1-0d13-44b5-99f6-595e0d4927aa"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "print(today)\n",
        "def plot_learning(history, experiment_name=\"No name\"):\n",
        "  HEIGHT = 1\n",
        "  WIDTH = 2\n",
        "  rate = 5.0\n",
        "  WpH_rate = 1.5\n",
        "  fig = plt.figure(figsize=(WIDTH*rate*WpH_rate, HEIGHT*rate))\n",
        "  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "  LOSS = fig.add_subplot(HEIGHT, WIDTH, 1) # loss, val_loss\n",
        "  ACC = fig.add_subplot(HEIGHT, WIDTH, 2) # acc, val_acc\n",
        "\n",
        "  # 1,1 loss\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  loss_props = {\n",
        "        'title' : 'Loss values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "  LOSS.set(**loss_props)\n",
        "  LOSS.plot(loss, label='loss', color='blue')\n",
        "  LOSS.plot(val_loss, label='val_loss', color='orange')\n",
        "  LOSS.legend(loc='best')\n",
        "\n",
        "  # 1,2 acc\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  acc_props = {\n",
        "        'title' : 'Accuracy values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "\n",
        "  ACC.set(**acc_props)\n",
        "  ACC.plot(acc, label='acc', color='blue')\n",
        "  ACC.plot(val_acc, label='val_acc', color='orange')\n",
        "  ACC.legend(loc='best')\n",
        "\n",
        "  #save\n",
        "  image_path = path.join(desk, experiment_name+today)\n",
        "  fig.savefig(image_path, bbox_inches='tight')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "IvhBOvD-hCMX",
        "outputId": "d9b4936d-d548-4672-9c44-302aa09f062e"
      },
      "source": [
        "plot_learning(histories[0], \"how_adam_used_for_KTH_in_certain_model\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57fZL4pkqsy"
      },
      "source": [
        "# 学習可視化データ保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDnwRrMtkqWi",
        "outputId": "5953eb4e-8ca9-4fc4-d899-4943a288ae84"
      },
      "source": [
        "shelf = '/content/drive/MyDrive/colab'\n",
        "book = 'tuning_results'\n",
        "shelf_book = os.path.join(shelf, book)\n",
        "shelf_book_page = path.join(shelf_book, today)\n",
        "print(shelf_book_page)\n",
        "# 保存\n",
        "if not os.path.exists(shelf_book_page):\n",
        "  os.makedirs(shelf_book_page)\n",
        "\n",
        "print(\"Please confirm in the desk\")\n",
        "print(\"OK? y/n\")\n",
        "IO = input()\n",
        "if IO == 'y':\n",
        "  print(\"writing all to a strage now.\")\n",
        "  for each_file in os.listdir(desk):\n",
        "    if re.match(r\"\\..*\", each_file,):\n",
        "      pass\n",
        "    elif re.match(r\".*\\.png\", each_file,):\n",
        "      print(\"->\", each_file)\n",
        "      shutil.copy2(each_file, shelf_book_page)\n",
        "    else:\n",
        "      pass\n",
        "else:\n",
        "  pass\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab/tuning_results/0121\n",
            "Please confirm in the desk\n",
            "OK? y/n\n",
            "y\n",
            "writing all to a strage now.\n",
            "-> how_adam_used_for_KTH_in_certain_model0121.png\n",
            "-> how_sgd_used_for_KTH_in_certain_model0121.png\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}