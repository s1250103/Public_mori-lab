{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examine_KTH_datasets.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/confirm_label_noize_for_cm_data/eras/confirm_label/examine_KTH_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn8mY7TNKCVG"
      },
      "source": [
        "#環境設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jJi-oO2J_on"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxGMERAvX3P8"
      },
      "source": [
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemsuBxCnpSD"
      },
      "source": [
        "# 実験条件（外乱）を定める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCV5Urc5PUH"
      },
      "source": [
        "LABEL_NOISE_RATE = 0.30\n",
        "TEST_DATA_RATE = 0.25\n",
        "EXPERIMENTS_NUMBER = 6"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FH7fepQ6Fo1"
      },
      "source": [
        "#学習条件を定める"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScShR1lmM_U"
      },
      "source": [
        "##学習手法の仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl9mQXfk7eS"
      },
      "source": [
        "seed = 20201218\n",
        "\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : 0.01,\n",
        "        \"momentum\" : 0.9,\n",
        "        \"decay\" : 1e-4,\n",
        "        \"nesterov\" : True\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 128,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()               \n",
        "    ]\n",
        "}\n",
        "\n",
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : 0.001,\n",
        "#         \"epsilon\" : 1e-8,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 128,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                         tf.keras.losses.MeanAbsoluteError(),\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "def compile_optimizer():\n",
        "  # 最適化処理 (adamのみ対応)\n",
        "  if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        epsilon=learningDict[\"optimizer\"][\"epsilon\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "    print(\"adam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "    optimizer = keras.optimizers.Nadam(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "        beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "        epsilon=None, \n",
        "        schedule_decay=0.4)\n",
        "    print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "  elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "    optimizer = keras.optimizers.SGD(\n",
        "        lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "        momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "        decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "        nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "    print(\"sgd is used as a optimizer\")\n",
        "  else:\n",
        "    print(\"error\")\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCt9_EOhkSt"
      },
      "source": [
        "## モデルの仕様（ニューラルネットワーク）\n",
        "<ul>\n",
        "  <li>入力層(フレームサイズ, フレームの高さ, フレームの横幅, RGB情報) </li>\n",
        "  <li>出力層(予測値) </li>\n",
        "  <li> 中間層 \n",
        "    <ol>\n",
        "      <li>conv0</li>\n",
        "      <li>pool0</li>\n",
        "      <li>conv1</li>\n",
        "      <li>pool1</li>\n",
        "      <li>dence0</li>\n",
        "  </li>\n",
        "</ui>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeda9UgKf23z"
      },
      "source": [
        "def make_model(video_format):\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (video_format.FRAME_SIZE,\n",
        "          video_format.HEIGHT,\n",
        "          video_format.WIDTH,\n",
        "          video_format.COLORinfo),\n",
        "          input_shape=(video_format.FRAME_SIZE * video_format.HEIGHT * video_format.WIDTH * video_format.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "  # 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  # pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool0'))\n",
        "\n",
        "  # 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(3, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "  # pool1\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 2, 2), name='pool1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='WATERSUPPLY')\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEbRDHGlku0r"
      },
      "source": [
        "##データの仕様"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SMSh_TkVjG"
      },
      "source": [
        "## フォーマットの設定\n",
        "class video_format:\n",
        "  # 想定された入力CMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から計算される値"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSHYRFpKdfa"
      },
      "source": [
        "# 学習データの用意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00VyH9N3Kl8h",
        "outputId": "48262a11-00e7-4884-8c74-03010cca79bf"
      },
      "source": [
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjO2EXNfnW1i"
      },
      "source": [
        "learning_data_path = \"/content/drive/MyDrive/colab/cleaned_detasets/KTH\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNUgCm4Ina0K",
        "outputId": "e6400651-6a11-4cb1-ca3f-f5ed70dd85bd"
      },
      "source": [
        "if path.isdir(learning_data_path):\n",
        "  print(\"actually exist the\", learning_data_path)\n",
        "  for each_data in os.listdir(learning_data_path):\n",
        "    if re.match(r\"Data.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as learning data.\")\n",
        "      learning_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    elif re.match(r\"Label.*\\.npz\", each_data):\n",
        "      print(\"________|------------ reading [\", each_data, \"] as label data.\")\n",
        "      label_data_np = np.load(path.join(learning_data_path, each_data))\n",
        "    else:\n",
        "      print(\"Not reading such data\", each_data)\n",
        "else:\n",
        "  print(\"no such path\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actually exist the /content/drive/MyDrive/colab/cleaned_detasets/KTH\n",
            "Not reading such data .ipynb_checkpoints\n",
            "________|------------ reading [ Data_of_KTH.npz ] as learning data.\n",
            "________|------------ reading [ Label_of_KTH.npz ] as label data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPqzDUMqokn"
      },
      "source": [
        "learning_data = []\n",
        "label_data = []\n",
        "for i in learning_data_np.files:\n",
        "  learning_data.append(learning_data_np[i])\n",
        "for i in label_data_np.files:\n",
        "  label_data.append(label_data_np[i])\n",
        "\n",
        "learning_data = np.array(learning_data)\n",
        "label_data = np.array(label_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzCVZtzp4d1"
      },
      "source": [
        "## 訓練データとテストデータとで分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHeiuRSl2ATW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(learning_data, label_data, random_state=20200120, train_size=(1-TEST_DATA_RATE))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DoTGxOV5Ach"
      },
      "source": [
        "## テストデータの教師ラベルに意図的なノイズを加える(実験のために)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMxF1H-645e"
      },
      "source": [
        "import random\n",
        "def rand_ints_nodup(a, b, k):\n",
        "  ns = []\n",
        "  while len(ns) < k:\n",
        "    n = random.randint(a, b)\n",
        "    if not n in ns:\n",
        "      ns.append(n)\n",
        "  return ns\n",
        "def changed_number(original_num, set_min, set_max):\n",
        "  while True:\n",
        "    tmpRndVal = random.randint(set_min, set_max)\n",
        "    if original_num != tmpRndVal:\n",
        "      return tmpRndVal"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6rf5RcI7AzI",
        "outputId": "5647882e-5e21-4d75-e4b1-7c742885bfc0"
      },
      "source": [
        "changed_label_number_for_experiment = int(len(Y_test) * LABEL_NOISE_RATE)\n",
        "change_points = rand_ints_nodup(0, len(Y_test)-1, changed_label_number_for_experiment )\n",
        "print(\"change map:\", sorted(change_points))\n",
        "print(\"the size:\", len(change_points))\n",
        "\n",
        "set_min = np.min(Y_test)\n",
        "set_max = np.max(Y_test)\n",
        "sum = 0\n",
        "if LABEL_NOISE_RATE != 0:\n",
        "  for i in range(len(Y_test)):\n",
        "    if i in change_points:\n",
        "      print(\"No.\", i, \", original number is\", Y_test[i], end=\" -> \")\n",
        "      Y_test[i] = changed_number(Y_test[i], set_min, set_max)\n",
        "      print(\"changed number is\", Y_test[i])\n",
        "      sum += 1\n",
        "    else:\n",
        "      pass\n",
        "else:\n",
        "  print(\"No label noizes\")\n",
        "  \n",
        "if sum == len(change_points):\n",
        "  print(\"changed correctlly.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "change map: [1, 5, 9, 16, 19, 24, 28, 40, 41, 42, 47, 52, 53, 54, 55, 61, 62, 66, 67, 72, 75, 76, 77, 82, 83, 84, 92, 93, 95, 98]\n",
            "the size: 30\n",
            "No. 1 , original number is 0 -> changed number is 1\n",
            "No. 5 , original number is 2 -> changed number is 3\n",
            "No. 9 , original number is 1 -> changed number is 2\n",
            "No. 16 , original number is 0 -> changed number is 2\n",
            "No. 19 , original number is 2 -> changed number is 3\n",
            "No. 24 , original number is 1 -> changed number is 3\n",
            "No. 28 , original number is 0 -> changed number is 2\n",
            "No. 40 , original number is 2 -> changed number is 3\n",
            "No. 41 , original number is 2 -> changed number is 0\n",
            "No. 42 , original number is 1 -> changed number is 2\n",
            "No. 47 , original number is 0 -> changed number is 3\n",
            "No. 52 , original number is 2 -> changed number is 0\n",
            "No. 53 , original number is 2 -> changed number is 0\n",
            "No. 54 , original number is 3 -> changed number is 1\n",
            "No. 55 , original number is 1 -> changed number is 0\n",
            "No. 61 , original number is 3 -> changed number is 0\n",
            "No. 62 , original number is 1 -> changed number is 0\n",
            "No. 66 , original number is 3 -> changed number is 0\n",
            "No. 67 , original number is 2 -> changed number is 0\n",
            "No. 72 , original number is 0 -> changed number is 3\n",
            "No. 75 , original number is 1 -> changed number is 3\n",
            "No. 76 , original number is 2 -> changed number is 3\n",
            "No. 77 , original number is 1 -> changed number is 3\n",
            "No. 82 , original number is 3 -> changed number is 1\n",
            "No. 83 , original number is 1 -> changed number is 2\n",
            "No. 84 , original number is 0 -> changed number is 3\n",
            "No. 92 , original number is 3 -> changed number is 1\n",
            "No. 93 , original number is 0 -> changed number is 1\n",
            "No. 95 , original number is 1 -> changed number is 3\n",
            "No. 98 , original number is 0 -> changed number is 1\n",
            "changed correctlly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM3OSjUWEPH3"
      },
      "source": [
        "##教師ラベルをone-hotに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHvnUv6KDty_"
      },
      "source": [
        "Y_train = tf.keras.utils.to_categorical(Y_train, 4)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, 4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjtSgxaGvBz"
      },
      "source": [
        "# 学習開始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3JzbydcYRv"
      },
      "source": [
        "def fix_loss_text(original):\n",
        "  result = re.sub(r\"<tensorflow\\.python\\.keras\\.losses\\.\", \"\", str(original))\n",
        "  result = re.sub(r\"\\sobject.+\", \"\", result)\n",
        "  return result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQGLjvFcLRA"
      },
      "source": [
        "used_losses_set = []\n",
        "for each_loss in learningDict[\"compared_losses\"]:\n",
        "  used_losses_set.append(fix_loss_text(each_loss))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXCHuDWpGxz-",
        "outputId": "f1829321-2e9c-495e-d5ec-99eb7f99833b"
      },
      "source": [
        "%%time\n",
        "obj_video_format = video_format()\n",
        "all_histories = [] # append EXPERIMENTS_NUMBER * used losses size\n",
        "for j in range(EXPERIMENTS_NUMBER):\n",
        "  seed_in_roop = seed + j\n",
        "  np.random.seed(seed_in_roop)\n",
        "  tf.random.set_seed(seed_in_roop)\n",
        "  massage = \"Start experiment in the No.\" + str(j+1) + \" loop.\"\n",
        "  print(massage)\n",
        "  print(\"also assigned[\", seed_in_roop, \"]as the seed. (the format is integer).\")\n",
        "  print(\"used loss functions\", len(used_losses_set), \"is :\", used_losses_set)\n",
        "  print(\"then notifying to certain line account\")\n",
        "  send_line_notify(massage)\n",
        "  print(\"runnng...\")\n",
        "\n",
        "  roop_histories = [] # append used losses size\n",
        "  for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "    massage = \"Try \" + fix_loss_text(each_loss) + \" the No.[\" + str(i+1) +\"]\"\n",
        "    print(massage)\n",
        "    send_line_notify(massage)\n",
        "\n",
        "    try:\n",
        "      # モデル構築\n",
        "      model = make_model(obj_video_format)\n",
        "      model.compile(\n",
        "            optimizer=compile_optimizer(),\n",
        "            loss=each_loss,\n",
        "            metrics=['acc'])\n",
        "      # 実行\n",
        "      \n",
        "      history = model.fit(\n",
        "            X_train, Y_train,\n",
        "            validation_data=(X_test, Y_test),\n",
        "            batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "            epochs=learningDict[\"theWay\"][\"epochs\"]\n",
        "            # verbose=0\n",
        "            )\n",
        "    except KeyboardInterrupt: \n",
        "      print(\"\\n\\nProcessing the KeyboardInterrupt\")\n",
        "\n",
        "    else:\n",
        "      roop_histories.append(history)\n",
        "      massage = \"Complete \" + fix_loss_text(each_loss) + \" the No.[\" + str(i+1) +\"]\"\n",
        "      print(massage)\n",
        "      send_line_notify(massage)\n",
        "    finally:\n",
        "      del model\n",
        "      keras.backend.clear_session()\n",
        "      gc.collect()\n",
        "      sleep(10)\n",
        "      print(\" and the model is erased.\")\n",
        "\n",
        "  #/for i\n",
        "  all_histories.append(roop_histories)\n",
        "  massage = \"Finish experiment in the No.\" + str(j+1) + \" loop.\"\n",
        "  print(massage)\n",
        "  print(\"then notifying to certain line account\")\n",
        "  send_line_notify(massage)\n",
        "\n",
        "#/for j\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start experiment in the No.1 loop.\n",
            "also assigned[ 20201218 ]as the seed. (the format is integer).\n",
            "used loss functions 3 is : ['CategoricalCrossentropy', 'MeanSquaredError', 'MeanAbsoluteError']\n",
            "then notifying to certain line account\n",
            "runnng...\n",
            "Try CategoricalCrossentropy the No.[1]\n",
            "sgd is used as a optimizer\n",
            "Epoch 1/128\n",
            "10/10 [==============================] - 9s 191ms/step - loss: 1.6303 - acc: 0.2265 - val_loss: 1.3846 - val_acc: 0.2900\n",
            "Epoch 2/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.3872 - acc: 0.1883 - val_loss: 1.3834 - val_acc: 0.2800\n",
            "Epoch 3/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 1.3825 - acc: 0.2549 - val_loss: 1.3838 - val_acc: 0.2800\n",
            "Epoch 4/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 1.3808 - acc: 0.3525 - val_loss: 1.3646 - val_acc: 0.3400\n",
            "Epoch 5/128\n",
            "10/10 [==============================] - 2s 158ms/step - loss: 1.3737 - acc: 0.2752 - val_loss: 1.3847 - val_acc: 0.2800\n",
            "Epoch 6/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3800 - acc: 0.3321 - val_loss: 1.3828 - val_acc: 0.2800\n",
            "Epoch 7/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 1.3892 - acc: 0.2263 - val_loss: 1.3839 - val_acc: 0.2800\n",
            "Epoch 8/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 1.3782 - acc: 0.2789 - val_loss: 1.3743 - val_acc: 0.2900\n",
            "Epoch 9/128\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.3655 - acc: 0.3181 - val_loss: 1.3534 - val_acc: 0.4000\n",
            "Epoch 10/128\n",
            "10/10 [==============================] - 1s 150ms/step - loss: 1.3013 - acc: 0.5040 - val_loss: 1.3817 - val_acc: 0.3200\n",
            "Epoch 11/128\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 1.3862 - acc: 0.2773 - val_loss: 1.3862 - val_acc: 0.2800\n",
            "Epoch 12/128\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 1.3856 - acc: 0.2659 - val_loss: 1.3857 - val_acc: 0.2800\n",
            "Epoch 13/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 1.3840 - acc: 0.2528 - val_loss: 1.3858 - val_acc: 0.2800\n",
            "Epoch 14/128\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 1.3852 - acc: 0.2765 - val_loss: 1.4086 - val_acc: 0.2800\n",
            "Epoch 15/128\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 1.3845 - acc: 0.2894 - val_loss: 1.3746 - val_acc: 0.2800\n",
            "Epoch 16/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 1.3659 - acc: 0.3044 - val_loss: 1.3552 - val_acc: 0.3500\n",
            "Epoch 17/128\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 1.3161 - acc: 0.4029 - val_loss: 1.3401 - val_acc: 0.3200\n",
            "Epoch 18/128\n",
            " 5/10 [==============>...............] - ETA: 0s - loss: 1.3248 - acc: 0.2698"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCUgTvVNE2V1"
      },
      "source": [
        "## @学習可視化（history を一つだけ見る）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237Plt6pA82q"
      },
      "source": [
        "IO = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypG238EdgVWy"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "print(today)\n",
        "def plot_learning(history, experiment_name=\"No name\"):\n",
        "  HEIGHT = 1\n",
        "  WIDTH = 2\n",
        "  rate = 5.0\n",
        "  WpH_rate = 1.5\n",
        "  fig = plt.figure(figsize=(WIDTH*rate*WpH_rate, HEIGHT*rate))\n",
        "  plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "  LOSS = fig.add_subplot(HEIGHT, WIDTH, 1) # loss, val_loss\n",
        "  ACC = fig.add_subplot(HEIGHT, WIDTH, 2) # acc, val_acc\n",
        "\n",
        "  # 1,1 loss\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  loss_props = {\n",
        "        'title' : 'Loss values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "  LOSS.set(**loss_props)\n",
        "  LOSS.plot(loss, label='loss', color='blue')\n",
        "  LOSS.plot(val_loss, label='val_loss', color='orange')\n",
        "  LOSS.legend(loc='best')\n",
        "\n",
        "  # 1,2 acc\n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  acc_props = {\n",
        "        'title' : 'Accuracy values plot',\n",
        "        'xlabel' : 'epoch',\n",
        "        'ylabel' : 'value'\n",
        "    }\n",
        "\n",
        "  ACC.set(**acc_props)\n",
        "  ACC.plot(acc, label='acc', color='blue')\n",
        "  ACC.plot(val_acc, label='val_acc', color='orange')\n",
        "  ACC.legend(loc='best')\n",
        "\n",
        "  #save\n",
        "  image_path = path.join(desk, experiment_name+today)\n",
        "  fig.savefig(image_path, bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YZZhYOBNQY"
      },
      "source": [
        "if IO:\n",
        "  plot_learning(histories[0], \"how_adam_used_for_KTH_in_certain_model\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w57fZL4pkqsy"
      },
      "source": [
        "## @学習可視化データ保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRGHTigrE-6F"
      },
      "source": [
        "if IO:\n",
        "  shelf = '/content/drive/MyDrive/colab'\n",
        "  book = 'tuning_results'\n",
        "  shelf_book = os.path.join(shelf, book)\n",
        "  shelf_book_page = path.join(shelf_book, today)\n",
        "  print(shelf_book_page)\n",
        "  # 保存\n",
        "  if not os.path.exists(shelf_book_page):\n",
        "    os.makedirs(shelf_book_page)\n",
        "\n",
        "  print(\"writing all to a strage now.\")\n",
        "  for each_file in os.listdir(desk):\n",
        "    if re.match(r\"\\..*\", each_file,):\n",
        "      pass\n",
        "    elif re.match(r\".*\\.png\", each_file,):\n",
        "      print(\"->\", each_file)\n",
        "      shutil.copy2(each_file, shelf_book_page)\n",
        "    else:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLC1pmB3FJQy"
      },
      "source": [
        "# 学習結果保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gjkqpp5bql"
      },
      "source": [
        "now_time = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "today = now_time.strftime('%m%d')\n",
        "\n",
        "for key_loss, each_loss in enumerate(used_losses_set):\n",
        "  print(each_loss)\n",
        "  for key_experient_number, each_experiment in enumerate(all_histories):\n",
        "    print(\"   \", end=\"\")\n",
        "    print(\"experient_number :\", key_experient_number)\n",
        "    each_history = all_histories[key_experient_number][key_loss]\n",
        "    hist_df = pd.DataFrame(each_history.history)\n",
        "\n",
        "    name = \"KTH\" + \".\"\n",
        "    name = name + str(LABEL_NOISE_RATE) + \".\"\n",
        "    name = name + each_loss + \".\"\n",
        "    name = name + str(key_experient_number) + \".\"\n",
        "    name = name + today\n",
        "    name = name + \".csv\"\n",
        "\n",
        "    hist_df.to_csv(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9WGL2AX-G5b"
      },
      "source": [
        "print(\"all in the desk:\")\n",
        "files_in_the_desk = sorted(os.listdir(desk))\n",
        "for each in files_in_the_desk:\n",
        "  print(\"_____\", end=\"\")\n",
        "  print(each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7v3R0ur-xYD"
      },
      "source": [
        "directory_name = \"noize=\" + str(LABEL_NOISE_RATE)\n",
        "shelf_path = \"/content/drive/MyDrive/colab/histories/KTH_robust\" \n",
        "shelf_path = path.join(shelf_path, directory_name)\n",
        "\n",
        "if not path.exists(shelf_path):\n",
        "  os.makedirs(shelf_path)\n",
        "else: pass\n",
        "\n",
        "for each_file in files_in_the_desk:\n",
        "  shutil.copy2(each_file, shelf_path)\n",
        "\n",
        "massage = \"Histories are written\"\n",
        "send_line_notify(massage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}