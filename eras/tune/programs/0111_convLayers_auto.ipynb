{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0111.convLayers_auto.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/eras/eras/tune/programs/0111_convLayers_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "lW6NHYqGmU-G",
        "outputId": "e93fbbf8-2e92-4100-bd72-085b62199692"
      },
      "source": [
        "#@title 前処理\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "\n",
        "## ツール\n",
        "def beep():\n",
        "  from google.colab import output\n",
        "  output.eval_js('new Audio(\\\n",
        "\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\")\\\n",
        ".play()') \n",
        "  \n",
        "## フォーマット規定\n",
        "class video_format:\n",
        "  name = \"video_format\"\n",
        "  # サンプリングされたCMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から導かれた値\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "format1 = video_format()\n",
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a7de3fb7718a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m## gdrive 接続\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Already confirm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed: invalid oauth code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m       raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed: invalid oauth code"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpHdixLUqQ1p"
      },
      "source": [
        "import requests\n",
        "\n",
        "  \n",
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    # data = {'message': f'message: {notification_message}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)\n",
        "\n",
        "\n",
        "    # send_line_notify('学習完了')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PRd1Kd2sRSd"
      },
      "source": [
        "#実験内容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH82LlSCHW0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd7b316-f506-40f7-9003-e02b9e4277ef"
      },
      "source": [
        "left = 1e-6\n",
        "right = 1e-7\n",
        "middle = (left + right) / 2\n",
        "\n",
        "middle = (middle + right) / 2\n",
        "\n",
        "print(\n",
        "    left > middle,\n",
        "    left < middle,\n",
        "    middle > right,\n",
        "    middle < right\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True False True False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIBWOv_UjDiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572b4e23-01c6-474c-be14-e3883223580c"
      },
      "source": [
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : middle,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 1024,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                          tf.keras.losses.CategoricalCrossentropy(),\n",
        "#                          tf.keras.losses.MeanSquaredError(), \n",
        "#     ]\n",
        "# }\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"Nadam\",\n",
        "        \"learning_rate\" : middle,\n",
        "        \"beta_1\" : 0.9,\n",
        "        \"beta_2\" : 0.999\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 32,\n",
        "        \"epochs\" : 1024,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                         tf.keras.losses.CategoricalCrossentropy(),\n",
        "                         tf.keras.losses.MeanSquaredError(), \n",
        "    ]\n",
        "}\n",
        "\n",
        "# 最適化処理 (adamのみ対応)\n",
        "if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "  optimizer = keras.optimizers.Adam(\n",
        "      lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "      beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "      beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "  print(\"adam is used as a optimizer\")\n",
        "elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "  optimizer = keras.optimizers.Nadam(\n",
        "      lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "      beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "      beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "      epsilon=None, \n",
        "      schedule_decay=0.004)\n",
        "  print(\"Nadam is used as a optimizer\")\n",
        "  \n",
        "else:\n",
        "  print(\"error\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nadam is used as a optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBa2QGgHVOn4"
      },
      "source": [
        "#データをインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYjxmEMpAZU1",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74542f43-94af-4551-ad5f-6ef448ed7b54"
      },
      "source": [
        "# 必要なファイルを'desk'にコピー\n",
        "%%time\n",
        "wants_paths = [\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTrainLabels.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTrainVideos.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTestLabels.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTestVideos.npz'\n",
        "]\n",
        "\n",
        "for want in wants_paths:\n",
        "  if not os.path.exists(os.path.join(desk, os.path.basename(want))):\n",
        "    shutil.copy2(want, desk)\n",
        "    print(\"get : \", want)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 292 µs, sys: 0 ns, total: 292 µs\n",
            "Wall time: 213 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z95hdLLJc-VD"
      },
      "source": [
        "# ファイルをメモリにコピー\n",
        "def prepare_data():\n",
        "  # traindata\n",
        "  v = np.load('/content/desk/normalTrainVideos.npz')\n",
        "  l = np.load('/content/desk/normalTrainLabels.npz')\n",
        "\n",
        "  train_videos = [] \n",
        "  train_labels = []\n",
        "  for i in v.files:\n",
        "    train_videos.append(v[i])\n",
        "  for i in l.files:\n",
        "    train_labels.append(l[i])\n",
        "\n",
        "  train_videos = np.array(train_videos)\n",
        "  train_labels = np.array(train_labels)\n",
        "  train_labels = tf.keras.utils.to_categorical(train_labels, 4)\n",
        "\n",
        "  # testdata\n",
        "  v = np.load('/content/desk/normalTestVideos.npz')\n",
        "  l = np.load('/content/desk/normalTestLabels.npz')\n",
        "\n",
        "  test_videos = []\n",
        "  test_labels = []\n",
        "  for i in v.files:\n",
        "    test_videos.append(v[i])\n",
        "  for i in l.files:\n",
        "    test_labels.append(l[i])\n",
        "\n",
        "  test_videos = np.array(test_videos)\n",
        "  test_labels = np.array(test_labels)\n",
        "  test_labels = tf.keras.utils.to_categorical(test_labels, 4)\n",
        "  \n",
        "  return train_videos, train_labels,  test_videos, test_labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_videos, train_labels,  test_videos, test_labels = prepare_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62PHow0YW7WT"
      },
      "source": [
        "#学習実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-fO9KmJcJo8"
      },
      "source": [
        "実験"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX4am7r0cPbC"
      },
      "source": [
        "seed = 20201218\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZwi2x9IexH5"
      },
      "source": [
        "%%time\n",
        "histories = []\n",
        "\n",
        "# 繰り返しの条件\n",
        "for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "  send_line_notify('学習開始 ' + str(i+1) + \"/\" + str(len(learningDict[\"compared_losses\"])))\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (format1.FRAME_SIZE,\n",
        "          format1.HEIGHT,\n",
        "          format1.WIDTH,\n",
        "          format1.COLORinfo),\n",
        "          input_shape=(format1.FRAME_SIZE * format1.HEIGHT * format1.WIDTH * format1.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "\n",
        "  ## 畳み込み0\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=32,\n",
        "          kernel_size=(2, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv0'))\n",
        "  ## pool0\n",
        "  model.add(\n",
        "      layers.MaxPooling3D(pool_size=(2, 3, 3), name='pool0'))\n",
        "\n",
        "  ## 畳み込み1\n",
        "  model.add(\n",
        "      layers.Conv3D(\n",
        "          filters=64,\n",
        "          kernel_size=(2, 3, 3),\n",
        "          strides=(1, 1, 1),\n",
        "          padding='same',\n",
        "          activation='relu',\n",
        "          name='conv1'))\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(1024,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='Output_Leayer')\n",
        "  )\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=each_loss,\n",
        "      metrics=['acc'])\n",
        "\n",
        "  # 実行\n",
        "  history = model.fit(\n",
        "        train_videos, train_labels,\n",
        "        validation_data=(test_videos, test_labels),\n",
        "        batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "        epochs=learningDict[\"theWay\"][\"epochs\"],\n",
        "        verbose=0\n",
        "        )\n",
        "  histories.append(history)\n",
        "  print(\"Complete.\")\n",
        "  \n",
        "\n",
        "  # 消去\n",
        "  sleep(60)\n",
        "  del model\n",
        "  keras.backend.clear_session()\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "beep()\n",
        "\n",
        "\n",
        "dt_now_jst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "now = dt_now_jst.strftime('%m%d-%H%M')\n",
        "csvs = []\n",
        "\n",
        "# cce\n",
        "hist_df = pd.DataFrame(histories[0].history)\n",
        "name = now + \"cce.csv\"\n",
        "hist_df.to_csv(name)\n",
        "csvs.append(name)\n",
        "# mse\n",
        "hist_df = pd.DataFrame(histories[1].history)\n",
        "name = now + \"mse.csv\"\n",
        "hist_df.to_csv(name)\n",
        "csvs.append(name)\n",
        "\n",
        "\n",
        "shelf = '/content/drive/MyDrive/colab'\n",
        "book = 'histories'\n",
        "shelf_book = os.path.join(shelf, book)\n",
        "shelf_book_page = os.path.join(shelf_book, now)\n",
        "\n",
        "if not os.path.exists(shelf_book_page):\n",
        "  os.mkdir(shelf_book_page)\n",
        "# 保存\n",
        "for one in csvs:\n",
        "  shutil.copy2(one, shelf_book_page)\n",
        "\n",
        "# 状況を保存\n",
        "for num, each in enumerate(learningDict[\"compared_losses\"]):\n",
        "  learningDict[\"compared_losses\"][num] = str(each)\n",
        "\n",
        "with open(\"situation.json\", 'w') as f:\n",
        "   json.dump(learningDict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "shutil.copy2(\"situation.json\", shelf_book_page)\n",
        "\n",
        "\n",
        "send_line_notify('学習完了：' + now)\n",
        "\n",
        "# # モデル画像保存\n",
        "# plot_model(model, \n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=False,\n",
        "#            to_file='model.png')\n",
        "# shutil.copy2(\"model.png\", shelf_book_page)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBOBDvoQudSL"
      },
      "source": [
        "おわり"
      ]
    }
  ]
}