{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0111.convLayers_auto.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1250103/Public_mori-lab/blob/eras/eras/tune/programs/0111_convLayers_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW6NHYqGmU-G",
        "cellView": "form",
        "outputId": "790b32e7-9a3b-4754-ae8a-a551ae7a65c5"
      },
      "source": [
        "#@title 前処理\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "## import\n",
        "# file dealing\n",
        "import os\n",
        "from os import path\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "# data dealing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "# process deasing\n",
        "import gc\n",
        "from time import sleep\n",
        "\n",
        "# machine learning (back)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import layers, models, initializers, callbacks\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import pprint\n",
        "import re\n",
        "import requests\n",
        "\n",
        "## ツール\n",
        "def beep():\n",
        "  from google.colab import output\n",
        "  output.eval_js('new Audio(\\\n",
        "\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\")\\\n",
        ".play()') \n",
        "\n",
        "def send_line_notify(notification_message):\n",
        "    \"\"\"\n",
        "    LINEに通知する\n",
        "    \"\"\"\n",
        "    line_notify_token = 'cHdELzsau6ve8hNVL3FxPz65Jdyquzuj2kd021u8q1L'\n",
        "    line_notify_api = 'https://notify-api.line.me/api/notify'\n",
        "    headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
        "    data = {'message': notification_message}\n",
        "    requests.post(line_notify_api, headers = headers, data = data)\n",
        "\n",
        "\n",
        "  \n",
        "## フォーマット規定\n",
        "class video_format:\n",
        "  name = \"video_format\"\n",
        "  # サンプリングされたCMデータの仕様\n",
        "  playtime = \"15秒\"\n",
        "  displaysize = \"(any, any, RGB)\"\n",
        "  videoformat = \"any\"\n",
        "  # モデルが扱うCMデータ(上のようなデータは、下のように変換される)\n",
        "  HEIGHT = 45\n",
        "  WIDTH = 80\n",
        "  FRAME_SIZE = 30\n",
        "  COLORinfo = 3 # \"RGB\"\n",
        "  FPS = \"2 (FRAME_SIZE / playtime)\" # 定義ではなく上から導かれた値\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "format1 = video_format()\n",
        "## gdrive 接続\n",
        "if not path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Already confirm\")\n",
        "\n",
        "## colab テンポラリディレクトリの作成\n",
        "desk = '/content/desk'\n",
        "if not os.path.exists(desk):\n",
        "  os.mkdir(desk)\n",
        "os.chdir(desk)\n",
        "print(\"Created at /content/desk\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already confirm\n",
            "Created at /content/desk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PRd1Kd2sRSd"
      },
      "source": [
        "#実験内容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIBWOv_UjDiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb06adae-e966-4954-b001-db7ed7cfb823"
      },
      "source": [
        "# learningDict = {\n",
        "#     \"optimizer\" : {\n",
        "#         \"this.optimizer\" : \"adam\",\n",
        "#         \"learning_rate\" : middle,\n",
        "#         \"beta_1\" : 0.9,\n",
        "#         \"beta_2\" : 0.999\n",
        "#     },\n",
        "#     \"theWay\" : {\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"epochs\" : 1024,\n",
        "#     },\n",
        "#     \"compared_losses\" : [\n",
        "#                          tf.keras.losses.CategoricalCrossentropy(),\n",
        "#                          tf.keras.losses.MeanSquaredError(), \n",
        "#     ]\n",
        "# }\n",
        "learningDict = {\n",
        "    \"optimizer\" : {\n",
        "        \"this.optimizer\" : \"sgd\",\n",
        "        \"learning_rate\" : 1e-5,\n",
        "        \"momentum\" : 0,\n",
        "        \"decay\" : 0.1\n",
        "        \"nesterov\" : False\n",
        "    },\n",
        "    \"theWay\" : {\n",
        "        \"batch_size\" : 4,\n",
        "        \"epochs\" : 1024,\n",
        "    },\n",
        "    \"compared_losses\" : [\n",
        "                        #  tf.keras.losses.CategoricalCrossentropy(),\n",
        "                        #  tf.keras.losses.MeanSquaredError(), \n",
        "                         tf.keras.losses.MeanAbsoluteError(),\n",
        "                        #  tf.keras.losses.SquaredHinge()\n",
        "                         \n",
        "    ]\n",
        "}\n",
        "\n",
        "# 最適化処理 (adamのみ対応)\n",
        "if learningDict[\"optimizer\"][\"this.optimizer\"] == \"adam\":\n",
        "  optimizer = keras.optimizers.Adam(\n",
        "      lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "      beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "      beta_2=learningDict[\"optimizer\"][\"beta_2\"])\n",
        "  print(\"adam is used as a optimizer\")\n",
        "\n",
        "elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"Nadam\":\n",
        "  optimizer = keras.optimizers.Nadam(\n",
        "      lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "      beta_1=learningDict[\"optimizer\"][\"beta_1\"],\n",
        "      beta_2=learningDict[\"optimizer\"][\"beta_2\"],\n",
        "      epsilon=None, \n",
        "      schedule_decay=0.4)\n",
        "  print(\"Nadam is used as a optimizer\")\n",
        "\n",
        "elif learningDict[\"optimizer\"][\"this.optimizer\"] == \"sgd\":\n",
        "  optimizer = keras.optimizers.SGD(\n",
        "      lr=learningDict[\"optimizer\"][\"learning_rate\"],\n",
        "      momentum=learningDict[\"optimizer\"][\"momentum\"],\n",
        "      decay=learningDict[\"optimizer\"][\"decay\"],\n",
        "      nesterov=learningDict[\"optimizer\"][\"nesterov\"]) \n",
        "  \n",
        "  print(\"sgd is used as a optimizer\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"error\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sgd is used as a optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBa2QGgHVOn4"
      },
      "source": [
        "#データをインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYjxmEMpAZU1",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb6c944-074a-4f8d-e191-5d2284634e17"
      },
      "source": [
        "# 必要なファイルを'desk'にコピー\n",
        "%%time\n",
        "wants_paths = [\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTrainLabels.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTrainVideos.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTestLabels.npz',\n",
        "'/content/drive/MyDrive/colab/dence1223/normalTestVideos.npz'\n",
        "]\n",
        "\n",
        "for want in wants_paths:\n",
        "  if not os.path.exists(os.path.join(desk, os.path.basename(want))):\n",
        "    shutil.copy2(want, desk)\n",
        "    print(\"get : \", want)\n",
        "            "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 189 µs, sys: 42 µs, total: 231 µs\n",
            "Wall time: 188 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z95hdLLJc-VD"
      },
      "source": [
        "# ファイルをメモリにコピー\n",
        "def prepare_data():\n",
        "  # traindata\n",
        "  v = np.load('/content/desk/normalTrainVideos.npz')\n",
        "  l = np.load('/content/desk/normalTrainLabels.npz')\n",
        "\n",
        "  train_videos = [] \n",
        "  train_labels = []\n",
        "  for i in v.files:\n",
        "    train_videos.append(v[i])\n",
        "  for i in l.files:\n",
        "    train_labels.append(l[i])\n",
        "\n",
        "  train_videos = np.array(train_videos)\n",
        "  train_labels = np.array(train_labels)\n",
        "  train_labels = tf.keras.utils.to_categorical(train_labels, 4)\n",
        "\n",
        "  # testdata\n",
        "  v = np.load('/content/desk/normalTestVideos.npz')\n",
        "  l = np.load('/content/desk/normalTestLabels.npz')\n",
        "\n",
        "  test_videos = []\n",
        "  test_labels = []\n",
        "  for i in v.files:\n",
        "    test_videos.append(v[i])\n",
        "  for i in l.files:\n",
        "    test_labels.append(l[i])\n",
        "\n",
        "  test_videos = np.array(test_videos)\n",
        "  test_labels = np.array(test_labels)\n",
        "  test_labels = tf.keras.utils.to_categorical(test_labels, 4)\n",
        "  \n",
        "  return train_videos, train_labels,  test_videos, test_labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_videos, train_labels,  test_videos, test_labels = prepare_data()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62PHow0YW7WT"
      },
      "source": [
        "#学習実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-fO9KmJcJo8"
      },
      "source": [
        "実験"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX4am7r0cPbC"
      },
      "source": [
        "seed = 20201218\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZwi2x9IexH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6aa071-5083-49ff-fba0-34a2e1a0cade"
      },
      "source": [
        "%%time\n",
        "histories = []\n",
        "\n",
        "# 繰り返しの条件\n",
        "for i, each_loss in enumerate(learningDict[\"compared_losses\"]):\n",
        "  send_line_notify('学習開始 ' + str(i+1) + \"/\" + str(len(learningDict[\"compared_losses\"])))\n",
        "  # モデル作成\n",
        "  model = models.Sequential()\n",
        "  # 入力層\n",
        "  model.add(\n",
        "      layers.Reshape(\n",
        "          (format1.FRAME_SIZE,\n",
        "          format1.HEIGHT,\n",
        "          format1.WIDTH,\n",
        "          format1.COLORinfo),\n",
        "          input_shape=(format1.FRAME_SIZE * format1.HEIGHT * format1.WIDTH * format1.COLORinfo,),\n",
        "          name='Input_Layer' )\n",
        "  )\n",
        "\n",
        "  # 畳み込み0\n",
        "  # model.add(\n",
        "  #     layers.Conv3D(\n",
        "  #         filters=128,\n",
        "  #         kernel_size=(1, 9, 16),\n",
        "  #         strides=(1, 9, 16),\n",
        "  #         padding='same',\n",
        "  #         activation='relu',\n",
        "  #         name='conv0'))\n",
        "  # # pool0\n",
        "  # model.add(\n",
        "  #     layers.MaxPooling3D(pool_size=(1, 3, 3), name='pool0'))\n",
        "\n",
        "\n",
        "  ## 全結合0\n",
        "  model.add(\n",
        "      layers.Flatten(name='pipe'),\n",
        "  )\n",
        "  model.add(\n",
        "      layers.Dense(128,\n",
        "        activation='relu',\n",
        "        name='dence0' ),\n",
        "  )\n",
        "  # model.add(\n",
        "  #     layers.Dense(128,\n",
        "  #       activation='relu',\n",
        "  #       name='dence1' ),\n",
        "  # )\n",
        "  # model.add(\n",
        "  #     layers.Dense(128,\n",
        "  #       activation='relu',\n",
        "  #       name='dence2' ),\n",
        "  # )\n",
        "  # 出力層\n",
        "  model.add(\n",
        "      layers.Dense(4, activation='softmax', name='Output_Leayer')\n",
        "  )\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=each_loss,\n",
        "      metrics=['acc'])\n",
        "\n",
        "  # 実行\n",
        "  history = model.fit(\n",
        "        train_videos, train_labels,\n",
        "        validation_data=(test_videos, test_labels),\n",
        "        batch_size=learningDict[\"theWay\"][\"batch_size\"],\n",
        "        epochs=learningDict[\"theWay\"][\"epochs\"],\n",
        "        verbose=0\n",
        "        )\n",
        "  histories.append(history)\n",
        "  print(\"Complete.\")\n",
        "  \n",
        "\n",
        "  # 消去\n",
        "  sleep(10)\n",
        "  del model\n",
        "  keras.backend.clear_session()\n",
        "  gc.collect()\n",
        "  send_line_notify('学習終了 ' + str(i+1) + \"/\" + str(len(learningDict[\"compared_losses\"])))\n",
        "\n",
        "\n",
        "# beep()\n",
        "\n",
        "\n",
        "# 状況を保存\n",
        "# for num, each in enumerate(learningDict[\"compared_losses\"]):\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # モデル画像保存\n",
        "# plot_model(model, \n",
        "#            show_shapes=True,\n",
        "#            show_layer_names=False,\n",
        "#            to_file='model.png')\n",
        "# shutil.copy2(\"model.png\", shelf_book_page)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete.\n",
            "CPU times: user 14min 30s, sys: 7min, total: 21min 31s\n",
            "Wall time: 21min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ3VKyS8OUS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a0873a-6216-4f67-df33-0d80747c1d34"
      },
      "source": [
        "import re\n",
        "def save_histories(histories):\n",
        "  dt_now_jst = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  now = dt_now_jst.strftime('%m%d-%H%M')\n",
        "  csvs = []\n",
        "\n",
        "  # もし実験回数が実験予定回数と同じなら\n",
        "  if len(histories) == len(learningDict[\"compared_losses\"]):\n",
        "    for i, each_history in enumerate(histories):\n",
        "      hist_df = pd.DataFrame(each_history.history)\n",
        "      used_loss = learningDict[\"compared_losses\"][i]\n",
        "      used_loss = str(used_loss)\n",
        "      used_loss = re.sub(\"<tensorflow.python.keras.losses.\", \"\", used_loss)\n",
        "      used_loss = re.sub(\"object at .*\", \"\", used_loss)\n",
        "      used_loss = re.sub(\"\\s\", \"\", used_loss)\n",
        "\n",
        "      name = str(used_loss) + \"_\" + now + \".csv\"\n",
        "      csvs.append(name)\n",
        "      print(name)\n",
        "      \n",
        "      hist_df.to_csv(name)\n",
        "      learningDict[\"compared_losses\"][i] = used_loss\n",
        "      \n",
        " \n",
        "    #/for i,\n",
        "    shelf = '/content/drive/MyDrive/colab'\n",
        "    book = 'histories'\n",
        "    shelf_book = os.path.join(shelf, book)\n",
        "    shelf_book_page = os.path.join(shelf_book, now)\n",
        "    print(shelf_book_page)\n",
        "\n",
        "    \n",
        "    # 保存\n",
        "    if not os.path.exists(shelf_book_page):\n",
        "      os.mkdir(shelf_book_page)\n",
        "    for one in csvs:\n",
        "      shutil.copy2(one, shelf_book_page)\n",
        "    \n",
        "    \n",
        "\n",
        "    with open(\"situation.json\", 'w') as f:\n",
        "      json.dump(learningDict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    shutil.copy2(\"situation.json\", shelf_book_page)\n",
        "    send_line_notify('保存完了：' + now)\n",
        "\n",
        "  #/if len(his\n",
        "  else:\n",
        "    send_line_notify('エラー：' + now)\n",
        "\n",
        "save_histories(histories)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MeanAbsoluteError_0116-1659.csv\n",
            "/content/drive/MyDrive/colab/histories/0116-1659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBOBDvoQudSL"
      },
      "source": [
        "おわり"
      ]
    }
  ]
}